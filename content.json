{"pages":[{"title":"블로그 역사","date":"2022-09-13T06:26:04.097Z","path":"about/index.html","text":"2021. 11. 20 블로그 역사 메뉴 추가 블로그 저자 메뉴 추가 2021. 11. 10 Github Action 추가하여 단일 배포 환경 구성 2021. 11. 09 블로그 개설"},{"title":"Categories","date":"2022-09-13T06:26:04.097Z","path":"categories/index.html","text":""},{"title":"Tags","date":"2022-09-13T06:26:04.097Z","path":"tags/index.html","text":""}],"posts":[{"title":"Face Detection","date":"2022-03-14T12:23:22.000Z","path":"wiki/face-detection/","text":"Face detection이란, 이미지 또는 동영상에서 사람의 얼굴 부분을 찾아서 bounding box를 그려주는 작업을 말한다. 본 문서에서는 이러한 face detection이 어떻게 이루어지는지, face detection을 위해 사용되는 feature가 어떤 것이 있는지를 논의한다. Overview of Face DetectionFace detection은 이미지 또는 동영상으로부터 사람의 얼굴 부분을 찾아서 bounding box를 찾아주는 알고리즘들을 가리킨다. 이미지나 동영상을 촬영할 때, 보통 가장 중요하게 생각되는 객체는 사람으로, 특히, 사람의 얼굴이 이미지나 동영상의 중심이 되는 경우가 많다. 따라서, 이미지나 동영상을 촬영하거나, 이미지를 처리할 때, 사람의 얼굴 부분을 탐지하고 사람의 얼굴을 중심으로 이미지나 동영상을 처리하는 어플리케이션이 많다. Face detection은 보통 다음과 같은 과정으로 이루어진다. 이미지(또는 비디오 프레임) 한 장을 입력으로 받는다. 슬라이딩 윈도우 기법으로 이미지 각 부분에 대해 feature를 추출한다. 계산된 feature를 바탕으로 이미지의 각 부분이 사람의 얼굴인지, 얼굴이 아닌지 판별한다. 사람의 얼굴이라고 판별된 부분의 bounding box를 리턴한다. 이때, 가장 중요한 부분이라고 할 수 있는 부분은 바로, 이미지의 각 부분에 대해 feature를 추출하는 것이라고 할 수 있다. 이때, feature는 사람의 얼굴 영역에서 나온 값과 사람의 얼굴이 아닌 영역에서 나온 값이 구분되는 feature여야 한다. 본 문서에서는 얼굴 탐지를 위해 가장 많이 사용되는 feature인 Haar feature를 살펴보고, Haar feature를 계산하기 위한 방법을 살펴본다. 여러 얼굴 이미지에 대해 Haar feature를 계산하고, 얼굴이 아닌 이미지에 대해 Haar feature가 계산되었다면, SVM과 같은 classifier를 학습할 수 있다. 학습된 SVM은 어떤 얼굴 이미지가 들어왔을 때, 얼굴이라고 판단할 수 있을 것이고, 얼굴이 아닌 이미지가 들어왔을 때에는 얼굴이 아니라고 판단할 수 있을 것이다. Haar Features for Face Detection이미지에서 feature라고 하면, edge, boundary 또는 SIFT와 같은 feature가 가장 먼저 떠오르게 된다. 사람의 얼굴 부분은 edge 및 boundary가 복잡하지만, edge와 boundary가 복잡한 객체는 얼마든지 많으므로, 얼굴을 탐지하는 feature로 사용하기에는 부적합하다. SIFT의 경우, 두 영역간 similarity를 비교하는데 주로 뛰어난 성능을 보여줄 뿐, 어떤 영역을 특정 카테고리로 판별하는 데에는 적합하지 않다. 따라서, 이미지의 어떤 영역이 얼굴인지를 판별하기 위해서는 조금 다른 feature가 필요한데, 보통 얼굴을 탐지하는 데 있어서 가장 널리 사용되는 feature는 Haar feature이다. Haar feature는 여러 스케일, 여러 가지의 Haar filter로 이미지의 각 영역에 적용한 후, 이들 필터들로 계산된 response의 합으로 계산된다. ‘$*$’ 표시는 convolution을 의미하며, filter에서 흰 부분은 +1 값, 검은 부분은 -1 값을 의미한다. 하나의 Haar feature는 하나의 Haar filter를 이미지에 convolution하여 얻어진다. Haar filter는 large-scale image derivative라고 볼 수 있는데, 다음의 Haar filter를 예로 들어보자. 이 Haar filter는 이미지에서 $x$축 방향의 edge에 민감하게 반응하는 edge detection filter(e.g. Sobel filter)와 유사한 모양이다. 즉, 이 Haar filter는 꽤 큰 크기의 $x$방향 edge detection filter라고 볼 수 있다. 마찬가지로, 다음의 Haar filter는 $y$축 방향으로의 큰 edge detection filter라고 볼 수 있다. 다음의 Haar filter의 경우, 2차 미분을 이용한 edge detector인 Laplacian filter와 유사한 역할이라고 볼 수 있다. Haar feature는 이처럼, 큰 스케일의 edge detection filter를 여러개 이용하여 추출한 feature를 의미한다. 이때, 다양한 스케일의 얼굴을 탐지하기 위해 위와 같은 Haar filter를 여러 스케일로 설계하게 된다. Computation Cost for Haar FeaturesFace detection은 카메라에 실시간으로 적용되는 경우가 많기에, 연산 비용이 적어야 한다. Haar feature를 계산하는 것은 convolution 곱을 직접 계산할 필요없이, (흰 영역의 픽셀 값들의 합) - (검은 영역의 픽셀 값들의 합)으로 간단하게 계산이 가능하기에 매우 효율적이다. 그럼에도 불구하고, Haar feature를 계산할 때, 모든 픽셀에 대해 계산해야 하며, Haar filter 역시 한두개가 아니다. 또한, Haar filter의 스케일도 다양하게 해야 하므로 그 연산 비용이 커지게 된다. Haar filter 하나에 대한 Haar feature 계산은 매우 효율적이지만, 전체적으로 보았을 때, 좀 더 속도를 개선할 필요가 있다. 이때, 사용될 수 있는 것이 integral image 방법이다. Integral ImageIntegral image란, 모든 픽셀좌표 $(i, j)$에 대해 다음과 같이 정의된다. $$II(i, j) = \\sum_{m=1}^i \\sum_{n=1}^j I(m, n)$$ 이때, $I(m, n)$은 $(m, n)$ 좌표에서의 이미지 픽셀값이고, $II(i, j)$는 $(i, j)$ 좌표에서의 integral image 값이다. 그림으로 표현하면 다음과 같다. Integral image에서 $(i, j)$ 좌표에 들어갈 값은 원본 이미지에서 $(1, 1)$ 좌표를 좌상단 꼭지점으로 하고, $(i, j)$를 우하단 꼭지점으로 하는 윈도우 내 모든 픽셀값들의 합이다. Haar feature를 계산하는 과정은 (흰 영역의 모든 픽셀값의 합) - (검은 영역의 모든 픽셀값의 합) 으로 계산되는데, Haar filter 윈도우 크기가 큰 경우, 더해야 하는 픽셀 개수가 많아지기 때문에 연산에 부담이 되었었다. 하지만, integral image가 있으면, 윈도우 크기와 상관없이 매우 적은 수의 덧셈으로 Haar feature를 계산할 수 있다. 예를들어, 어떤 이미지에서, 다음 윈도우 크기 내 모든 픽셀값들의 합을 계산하고 싶다고 하자. 이 윈도우 내의 모든 픽셀값들의 합을 계산하는 과정은 픽셀 개수가 늘어남에 따라 연산량이 변한다. 또한, Haar filter 윈도우 크기는 기본적으로 크기 때문에, 연산해야 하는 덧셈 수는 매우 많다. 하지만, integral image가 있는 경우, 이야기가 달라진다. Integral image가 있는 경우, 위 그림에서의 윈도우 내의 모든 픽셀값을 계산하는 과정은 단 3번의 덧셈으로 가능하다. 윈도우 내의 모든 픽셀값의 합은 다음 integral image에서, $(a - b - c + d)$와 같다. 윈도우 내의 모든 픽셀값들의 합은 이처럼 단 세번의 덧셈으로 가능하며, 윈도우 크기가 크더라도 덧셈은 3번이면 충분하다. 다음 Haar filter를 이용하여 Haar feature를 계산하는 과정은 일곱번의 덧셈이면 충분하다. 위 그림에서 왼쪽 그림의 Haar filter를 이용하여 Haar feature를 계산하는 과정은, integral image에서 다음을 계산하는 것과 같다. $$\\text{Haar Feature} = (b - c - e + f) - (a - b - d + e)$$ Integral image가 있으면 Haar feature를 매우 적은 수의 덧셈으로 계산이 가능하다는 것을 알았는데, Integral image를 계산하는 과정이 복잡하면 의미가 없다. 하지만, integral image 역시 매우 빠르게 효율적으로 계산이 가능하다. Integral Image ComputationIntegral image를 계산하는 과정은 다음과 같다. Integral image $II$의 모든 픽셀값을 0으로 초기화한다. Integral image의 $(1, 1)$ 좌표부터 $(h, w)$ 좌표(이미지 우하단 좌표)까지 왼쪽에서 오른쪽으로, 위에서 아래로 훑으면서 다음을 실행한다. $$II(i, j) = I(i, j) + II(i - 1, j) + II(i, j - 1) - II(i - 1, j - 1)$$ 그림으로 표현하면 다음과 같다. 연한 노랑색 부분은 이미 계산이 완료된 부분이며, 현재 $a$ 픽셀값에서의 integral image 값을 계산하고 있다고 하자. 이때, $b, c, d$는 이미 integral image 값이 계산이 완료되었다. $a$ 위치에서의 integral image 값은 다음과 같이 계산된다. $$II(a) = I(a) + II(b) + II(c) - II(d)$$ 즉, 이미지의 모든 픽셀을 한번씩 쭉 훑으면 integral image가 계산된다. Integral image는 한 번 계산해놓으면 모든 Haar feature 계산에 이용할 수 있어서 매우 효율적이다.","tags":[{"name":"Computer Vision","slug":"Computer-Vision","permalink":"https://the-masked-developer.github.io/tags/Computer-Vision/"}],"categories":[{"name":"Computer Vision","slug":"Computer-Vision","permalink":"https://the-masked-developer.github.io/categories/Computer-Vision/"}]},{"title":"Binary Image Processing (Geometric Information)","date":"2022-01-23T03:23:22.000Z","path":"wiki/binary-image-geometric-process/","text":"# Geometric Information바이너리 이미지에서 추출할 수 있는 첫 번째 주요 정보는 객체에 대한 geometric 정보이다. 본 문서는 바이너리 이미지에서 추출할 수 있는 geometric 정보에 대해 다룬다. Geometric Information of Binary Images바이너리 이미지로 얻을 수 있는 이미지의 특징 중 하나는 이미지 내의 객체에 대한 geomatric 정보이다. 여기서, 객체의 geometric 정보란 다음과 같은 정보들을 말한다. 이미지 내에서 객체의 넓이(area) 이미지 내에서 객체의 위치 좌표(coordinates) 이미지 내에서 객체의 방향(direction) 이미지를 binarization할 수 있다면, 위와 같은 이미지의 geometric 특징을 추출할 수 있다. Area of ObjectsGray-scale 이미지 $g$에 대해 추출한 바이너리 이미지를 $b$라고 할때, 이미지 속 객체의 넓이(area) $A$는 다음과 같이 계산할 수 있다. $$A = \\int \\int b(x, y) dx dy$$ 이때, $b(x, y)$는 $(x, y)$좌표에서의 픽셀의 바이너리 값으로 객체에 해당하는 픽셀만 1, 나머지 영역은 0이 된다. 하지만, 이미지 픽셀 사이의 공간은 discrete 이므로, 다음과 같이 객체의 넓이를 계산해야 한다. $$A = \\sum_{x} \\sum_{y} b(x, y)$$ 객체의 넓이는 이미지 내에서 객체가 움직이던, 회전하던 변하지 않는 특징이 될 수 있다(3차원적으로 회전하는 것 제외). 그러나, 이미지 내에 객체가 여러개인 경우, classification(or segmentation)을 먼저 수행할 필요가 있다. 참고로, 바이너리 이미지에서 객체의 넓이는 zero moment에 해당한다. Position of Objects바이너리 이미지 $b$에서, 객체에 해당하는 픽셀이 모두 1이고, 배경에 해당하는 픽셀이 모두 0일때, 객체의 중심 좌표는 다음과 같이 계산한다. $$\\bar{x} = \\frac{1}{A} \\int \\int x b(x, y) dx dy \\\\bar{y} = \\frac{1}{A} \\int \\int y b(x, y) dx dy$$ 이때, $A$는 객체의 넓이로, 바이너리 이미지에서 객체에 해당하는 픽셀의 개수이다. 역시, 픽셀공간이 discrete이기 때문에 다음과 같이 객체의 위치를 계산해야 한다. $$\\bar{x} = \\frac{1}{A} \\sum_x \\sum_y x b(x, y) \\\\bar{y} = \\frac{1}{A} \\sum_x \\sum_y y b(x, y)$$ 객체의 위치는 first moment에 해당한다. Direction of Objects객체의 방향이란, 객체의 모양에서 가장 긴 축을 말한다. 예를들어, 다음 항아리 모양의 객체에서 객체의 방향은 그림의 축과 같다. 바이너리 이미지에서 객체의 방향을 구하는 아이디어는 “2nd central moment가 가장 작은 축”을 구하는 것이다. 2nd central moment란, 어떤 축이 있을 때, 해당 축과 각 픽셀과의 거리의 제곱합을 의미하며, $(x, y)$좌표에 있는 점과 축과의 거리를 $r_{x, y}$라고 했을 때, 다음과 같이 정의된다. $$E = \\int \\int r_{x,y}^2 b(x, y) dx dy$$ 다음과 같은 두 개의 축을 생각해보자. 왼쪽 그림에서는 객체의 모든 픽셀이 축과의 거리가 다 가까우므로 $E$ 값이 작다. 반면, 오른쪽 그림에서는 객체의 일부 픽셀이 축과의 거리가 상당히 먼 경우가 많기 때문에, $E$값이 커지게 된다. 즉, $E$값이 최소가 되는 축을 찾으면 객체의 방향 축을 찾을 수 있게 된다(PCA가 생각나는데?). 우리가 어떤 직선을 정의할때 보통 다음과 같이 직선의 방정식을 이용하게 된다. $$y = mx + b$$ 그러나, 이런 형태의 식에서는 $m$의 범위가 $-\\infty$에서 $\\infty$가 되기 때문에, 최적화가 좀 어렵다. 따라서, 위와 같은 직선의 방정식 말고 직선의 각도와 원점과의 거리를 이용하여 직선을 reparameterization하여 다음과 같이 정의한다. $$x \\sin \\theta - y \\cos \\theta + \\rho = 0$$ 이때, $\\theta$는 해당 직선과 $x$축 사이의 각도이며, $\\rho$는 직선과 원점(0, 0) 사이의 거리이다. 이 직선과 $(x, y)$ 좌표의 픽셀 까지의 거리 $r_{x, y}$는 다음과 같다. $$r_{x, y} = \\frac{|x \\sin \\theta - y \\cos \\theta + \\rho|}{\\sqrt{\\sin^2 \\theta + \\cos^2 \\theta}} = |x \\sin \\theta - y \\cos \\theta + \\rho|$$ 이에 따라, 2nd central moment $E$는 다음과 같이 변형될 수 있다. $$E = \\int \\int |x \\sin \\theta - y \\cos \\theta + \\rho|^2 b(x, y) dx dy$$ 우리가 원하는 것은 이 $E$를 최소화하는 축을 찾는 것이므로, $E$를 최소화하는 $\\theta$와 $\\rho$를 찾는 것이 우리가 계산해야 할 것이다. 위 $E$를 $\\rho$에 대해 미분하여 0이 되는 “극점”을 찾아보자. $$\\frac{d}{d\\rho} E = \\int \\int 2 |x \\sin \\theta - y \\cos \\theta + \\rho| b(x, y) dx dy = 0 \\= 2 (A \\bar{x} \\sin \\theta - A \\bar{y} \\cos \\theta + A \\rho) = 0 \\2A (\\bar{x} \\sin \\theta - \\bar{y} \\cos \\theta + \\rho) = 0$$ 이때, $A$는 객체의 넓이이고, $\\bar{x}, \\bar{y}$는 객체의 중심점 좌표이다. 객체의 넓이는 0이 아니므로, 직선 $x \\sin \\theta - y \\cos \\theta + \\rho = 0$ 이 객체의 방향 축이 되려면 다음을 만족해야 한다. $$\\bar{x} \\sin \\theta - \\bar{y} \\cos \\theta + \\rho = 0$$ 즉, 직선 $x \\sin \\theta - y \\cos \\theta + \\rho = 0$는 객체의 중심 $(\\bar{x}, \\bar{y})$를 지나는 직선이어야 한다. 이것을 반영하여 직선을 다음과 같이 수정하자. $$(x - \\bar{x}) \\sin \\theta - (y - \\bar{y}) \\cos \\theta = 0$$ 이때, $E$를 다시 적어보면 다음과 같다. $$E = \\int \\int \\vert x \\sin \\theta - y \\cos \\theta + \\rho \\vert^2 b(x, y) dx dy$$ $$= \\int \\int \\vert(x - \\bar{x}) \\sin \\theta - (y - \\bar{y}) \\cos \\theta \\vert^2 b(x, y) dx dy$$ $$= \\int \\int ((x - \\bar{x})^2 \\sin^2 \\theta + (y - \\bar{y})^2 \\cos^2 \\theta - 2(x - \\bar{x})(y - \\bar{y})\\sin \\theta \\cos \\theta) b(x, y) dx dy$$ $$= \\sin^2 \\theta \\int (x - \\bar{x})^2 dx+ \\cos^2 \\theta \\int (y - \\bar{y})^2 dy- 2 \\sin \\theta \\cos \\theta \\int \\int (x - \\bar{x})(y - \\bar{y}) dx dy$$ 이때, $\\bar{x}, \\bar{y}$는 쉽게 구할 수 있으므로, 맨 마지막 식의 integral은 간단하게(?) 계산이 가능하다. $E$식을 좀 간단하게 적기 위해 다음을 정의하자. $$a := \\int (x - \\bar{x})^2 dx \\b := \\int (y - \\bar{y})^2 dy \\c := \\int \\int (x - \\bar{x})(y - \\bar{y}) dx dy$$ 이때, $E$는 다음과 같이 변형할 수 있다. $$E = a \\sin^2 \\theta + b \\cos^2 \\theta - 2c \\sin \\theta \\cos \\theta$$ 이제, 이 식을 다시 $\\theta$에 대해 미분하여 0이 되는 지점을 찾아 $E$의 “극점”을 구해보자. $$\\frac{d}{d\\theta} E = 2a\\sin\\theta \\cos \\theta - 2b \\sin \\theta \\cos \\theta - 2c(\\cos^2 \\theta - \\sin^2 \\theta) = 0 \\= (a - b) \\sin 2\\theta - 2c \\cos 2\\theta = 0 \\\\tan 2 \\theta = \\frac{2c}{a - b}$$ 즉, 마지막 식이 만족할 때, $E$가 최소 또는 최대가 된다. 그런데, $0 \\leq \\theta \\leq 2\\pi$ 범위에서, 방정식의 해는 2개가 되는데, 어떤 $\\theta_1$에 대해, $\\tan 2 \\theta_1 = \\tan (2 \\theta_1 + \\pi)$ 이기 때문이다. 이때, 하나는 최소에 대한 해, 하나는 최대에 대한 해일 것인데, 2차 미분을 통해 어느 점이 최소점인지 찾아야 한다. $E$의 2차 미분은 다음과 같다. $$\\frac{d^2}{d\\theta^2} E = 2(a - b) \\cos 2\\theta + 4c \\sin 2\\theta$$ 이 식에 $\\theta_1$와 $\\theta_1 + \\frac{1}{2}\\pi$를 넣어보고, 2차미분값이 양의 값인 부분인 $\\theta$를 찾으면 된다. 여기까지, 직선의 방정식의 파라미터 $\\rho, \\theta$를 구한 것이며, 객체의 방향 축을 구하는 과정이었다. 결과적으로, $\\theta$는 다음과 같다. $$\\theta = \\frac{1}{2}\\arctan\\frac{2c}{a - b}$$ ## Roundness객체가 얼마나 둥글둥글한지를 측정하는 지표로, 2nd central moment $E$의 최댓값 $E_{\\max}$과 최솟값 $E_{\\min}$으로 구할 수 있다. $$\\text{roundness} = \\frac{E_{\\min}}{E_{\\max}}$$ 이 값이 1에 가까우면 객체가 둥글둥글 한 것이고, 0에 가까우면 객체가 길쭉한 타원에 가까운 것이다.","tags":[{"name":"Computer Vision","slug":"Computer-Vision","permalink":"https://the-masked-developer.github.io/tags/Computer-Vision/"}],"categories":[{"name":"Computer Vision","slug":"Computer-Vision","permalink":"https://the-masked-developer.github.io/categories/Computer-Vision/"}]},{"title":"Intro to Binary Images","date":"2022-01-23T03:22:22.000Z","path":"wiki/intro-to-binary-image/","text":"# Introduction to Binary Images바이너리 이미지란, 한개 또는 두 개의 숫자로만 이미지 값으로 존재하는 이미지를 말한다. 흔히, 완전 흰색과 완전 흑색으로만 구성되기도 한다. 바이너리 이미지는 다루기 쉬우면서도 이미지로부터 추출할 수 있는 매우 강건한 특징(feature)중 하나이다. 바이너리 이미지를 일단 얻게 되면, 이미지를 segmentation 하거나 분류하는 등의 작업을 수행할 수 있다. Binary Images바이너리 이미지는 하나 또는 두 개의 값(보통 0과 1)으로 구성된 이미지이다. 바이너리 이미지 $b$는 gray-scale 이미지 $g$를 thresholding을 수행함으로서 얻을 수 있다. $g(x, y) &lt; T$ 이면, $b(x, y) = 0$ $g(x, y) \\geq T$ 이면, $b(x, y) = 1$ 바이너리 이미지는 이미지 속 객체의 structure를 파악하는데 매우 중요한 역할을 할 수 있으며, 객체 segmentation을 매우 뛰어나게 수행할 수 있다는 장점이 있다. 하지만, threshold $T$를 주의깊게 설정해야 하는데, 너무 낮게 설정하면 모두 흰색인 바이너리 이미지가 얻어질 것이고, 너무 높게 설정하면 모두 검은색인 바이너리 이미지가 얻어질 것이다. Threshold를 정하는 방법은 임의로 정하는 방법도 있지만, color histogram을 이용하는 방법도 있다. Color histogram을 이용하여 threshold를 정하는 방법은 이미지에서 foreground와 background가 명확하게 구분되는 이미지에서만 가능한데, 이러한 이미지에서 color histogram을 그린 후, color histogram의 골짜기 부분을 threshold로 삼는 것이다. 골짜기 부분을 threshold로 삼게 되면 foreground 객체는 하얗게, background 객체는 검게 binarize될 것이다. 이미지로부터 바이너리 이미지를 얻어내는 작업은 때로 어려울 수 있다. 배경이 다양하거나 객체의 색상 또는 명암이 배경과 비슷한 경우, threshold를 하나로 정하기 힘든 경우가 많다. 이 경우, 바이너리 이미지를 얻는 것 대신 다른 방법으로 이미지의 특징을 추출할 필요가 있다. 이처럼 바이너리 이미지를 계산할 수 있는 경우는 한정적이지만, 바이너리 이미지를 계산할 수 있다면, 매우 강건한 이미지의 특징을 얻을 수 있다. 바이너리 이미지로부터 추출할 수 있는 중요한 특징은 다음과 같이 크게 세 가지가 존재한다. 객체에 대한 geometric 정보 객체에 대한 세그멘테이션 마스크 객체에 대한 구조적 정보(스켈레톤 등) 이후 문서에서 위 세 가지 정보를 추출하는 방법에 대해 다루고자 한다. Binary Image Processing (Geometric Information)","tags":[{"name":"Computer Vision","slug":"Computer-Vision","permalink":"https://the-masked-developer.github.io/tags/Computer-Vision/"}],"categories":[{"name":"Computer Vision","slug":"Computer-Vision","permalink":"https://the-masked-developer.github.io/categories/Computer-Vision/"}]},{"title":"업무시간에 몰래 작성해보는 springboot jpa entity와 dto","date":"2021-12-15T08:50:00.000Z","path":"wiki/[springboot]jpa-dto-basic/","text":"JPA, JpaRepository 소개 JPA란? Java ORM 기술 표준으로 사용하는 인터페이스 모음 JpaRepository란? Spring, SpringBoot Framework에서 제공하는 인터페이스 QueryByExampleExecutor와 PagingAndSortingRepository를 상속함 기본적인 수준의 검색 메서드가 정의되어 있어서 override하여 사용 가능 123456789101112131415161718192021222324public interface JpaRepository&lt;T, ID&gt; extends PagingAndSortingRepository&lt;T, ID&gt;, QueryByExampleExecutor&lt;T&gt; &#123; /* * (non-Javadoc) * @see org.springframework.data.repository.CrudRepository#findAll() */ @Override List&lt;T&gt; findAll(); /* * (non-Javadoc) * @see org.springframework.data.repository.PagingAndSortingRepository#findAll(org.springframework.data.domain.Sort) */ @Override List&lt;T&gt; findAll(Sort sort); /* * (non-Javadoc) * @see org.springframework.data.repository.CrudRepository#findAll(java.lang.Iterable) */ @Override List&lt;T&gt; findAllById(Iterable&lt;ID&gt; ids); ... Entity와 Dto 다음과 같은 테이블 Coupon 있다고 하자 Column Type Default Value Nullable 그 외 no int NO Primary Key, Auto Increment coupon_no varchar NO 쿠폰번호 use_date timestamp YES 사용일자 use_flag enum(‘Y’,’N’) N NO 사용여부 discount int 0 NO 할인가격 description varchar YES 쿠폰 설명 discount_type enum(‘P’, ‘W’) W NO 할인 방식 (‘P’ : 퍼센티지, ‘W’ : 원) create_date timestamp CURRENT_TIMESTAMP NO Default Generated 생성 일자 Entity는 해당 테이블과 매핑하여 사용Dto는 테이블에서 받은 Entity를 다른 layer로 전송할때 사용 (Data Transfer Object) entity와 dto는 다음과 같이 만들 수 있다. ENTITY 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package entity;import enums.DiscountType;import enums.YorN;import lombok.*;import org.hibernate.annotations.CreationTimestamp;import javax.persistence.*;import java.time.LocalDateTime;@ToString@Builder@AllArgsConstructor@NoArgsConstructor@Getter@Setter@Entity@Table(name=&quot;coupon&quot;)public class Coupon &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) @Column(name = &quot;no&quot;) private int no; @Column(name = &quot;coupon_no&quot;) private String couponNo; @Column(name = &quot;use_date&quot;) @Builder.Default private LocalDateTime useDate = null; @Enumerated(EnumType.STRING) // EnumType은 ORDINAL과 STRING이 있음 @Builder.Default // Builder로 생성 시 default 값 설정 @Column(name = &quot;use_flag&quot;) private YorN useFlag = YorN.N; // 설정될 default값 (&#x27;N&#x27;) @Column(name = &quot;discount&quot;) @Builder.Default private int discount = 0; @Column(name = &quot;description&quot;, nullable = true) private String description; @Enumerated(EnumType.STRING) @Column(name = &quot;discount_type&quot;) @Builder.Default private DiscountType discountType = DiscountType.W; @CreationTimestamp @Column(name= &quot;create_date&quot;, columnDefinition = &quot;TIMESTAMP&quot;) private LocalDateTime createDate;&#125; DTO 1234567891011121314151617181920212223242526272829303132333435package dto;import enums.DiscountType;import enums.YorN;import entity.Coupon;import lombok.*;import java.time.LocalDateTime;@Getter@Setter@AllArgsConstructor@NoArgsConstructorpublic class CouponDto &#123; private int no; private String couponNo; private LocalDateTime useDate; private YorN useFlag; private int discountPrice; private DiscountType discountType; private String description; private LocalDateTime createDate; @Builder(builderMethodName = &quot;CouponDtoBuilder&quot;) public CouponDto(Coupon coupon)&#123; no = coupon.getNo(); couponNo = coupon.getCouponNo(); useDate = coupon.getUseDate(); useFlag = coupon.getUseFlag(); discountPrice = coupon.getDiscount(); discountType = coupon.getDiscountType(); description = coupon.getDescription(); createDate = coupon.getCreateDate(); &#125;&#125; ENUM 123456789101112131415161718package enums;import com.fasterxml.jackson.annotation.JsonValue;import java.util.Locale;public enum DiscountType &#123; P, W; @JsonValue public String getStatus() &#123; return this.name().toUpperCase(Locale.ROOT); &#125;&#125;public enum YorN &#123; Y, N; @JsonValue public String getStatus() &#123; return this.name().toUpperCase(Locale.ROOT); &#125;&#125; @Enumerated(EnumType)의 경우, ORDINAL과 STRING이 존재 ORDINAL - P : 1, W : 2 … enum의 순서가 DB에 저장됨 STRING - P, W … enum의 문자열 값이 DB에 저장됨 JpaRepository를 상속하는 CouponRepository를 다음과 같이 생성하고 1234567891011package repository;import entity.Coupon;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.stereotype.Repository;@Repositorypublic interface CouponRepository extends JpaRepository&lt;Coupon, Integer&gt; &#123; Coupon findCouponByNo(int no);&#125; Service Layer에서는 다음과 같이 DTO로 사용할 수 있다. 12345678910111213141516171819202122232425package service;import dto.CouponDto;import entity.Coupon;import repository.CouponRepository;import lombok.RequiredArgsConstructor;import org.springframework.stereotype.Service;@Service@RequiredArgsConstructorpublic class CouponService &#123; private final CouponRepository couponRepository; public CouponDto getCouponInfoByNo(int no) &#123; Coupon coupon = couponRepository.findCouponByNo(no); CouponDto couponDto = CouponDto.CouponDtoBuilder().coupon(coupon).build(); // Do Something return couponDto; &#125;&#125;","tags":[{"name":"java","slug":"java","permalink":"https://the-masked-developer.github.io/tags/java/"},{"name":"springboot","slug":"springboot","permalink":"https://the-masked-developer.github.io/tags/springboot/"}],"categories":[{"name":"java","slug":"java","permalink":"https://the-masked-developer.github.io/categories/java/"},{"name":"springboot","slug":"java/springboot","permalink":"https://the-masked-developer.github.io/categories/java/springboot/"}]},{"title":"RESTful API design.","date":"2021-12-02T05:11:45.000Z","path":"wiki/restful-api-design/","text":"RESTful API design’s Goals. Platform indenpendency. Self-evolution. 굳이 HTTP 일 필요는 없다. design 의 문제. Design Principles. Resource 를 중심으로 설계된다. Resource 는 고유의 identifier 를 가진다. Representation 을 가진다. 보통 JSON. 각 Request 는 Stateless, Atomic, Indenpendent 하다. Server 의 scale out 을 가능케 한다. Indenpendent 하기 때문에 요청의 순서에 신경쓰지 않아도 된다. Maturity of RESTful HTTP API. POST only. Resource + URI 분리 Resource + HTTP methods. HATEOAS (+hyperlinks) 보통, 대부분의 RESTful HTTP API 는 2 ~ 3 그 어딘가에 있다. Design guide. No verbs. (Use methods for it!) There’s no need to match with database schema. (just abstract it. hide implementation.) resource 의 collection 들은 다른 URI 를 가진다. ex) /orders Resource 의 relationship 은 hierachy 하게 표현한다. ex) /customers/&#123;customer_id&#125;/orders/&#123;order_id&#125;… 관계가 너무 많으면 관리가 힘들다. 그러므로 여러번 요청을 날린다던지 너무 계층이 많아지지 않도록 조심한다. chatty 한 API design 보다는, 하나의 큰 load 를 가진 API 가 낫다. (denormalized API is better) 지나친 Overfetching 은 조심해야 한다. Abstract database schema. do not expose it. Resource 와 관련없는 서버의 function, procedure 를 실행해야 할 경우도 있다. 적당히 사용한다. HTTP methods. GET: Resource’s representation. 200 404 POST: Resource’s creation or call server’s function. 201: creation success. 200: no creation. 400: bad representation. 생성시 Location Header 에 resource 의 ID 를 담아서 리턴한다. PUT: Create or update resource (w/ complete representation) 201, 200, 400 409(Conflict): Valid 한 요청이지만, Server 의 state 로 인해 update 를 실패했다. 생성의 경우 Client 가 적절하게 좋은 ID 를 생성할 수 있어야 한다. PATCH: Update resource partially 200, 400, 409 Representation fields are optional. null means delete the field’s value. DELETE: Delete resource. 204(No Content): success Filter and PaginationTBW. https://docs.microsoft.com/en-us/azure/architecture/best-practices/api-design#filter-and-paginate-data Versioning No versioning. acceptable for internal APIs. Header versioning. ex) App-Version: ... Query string versioning. ex) /...?version=1 URI versiong. ex) /v1/.... media type versioning. Open APITBW. https://docs.microsoft.com/en-us/azure/architecture/best-practices/api-design#open-api-initiative References Microsoft RESTful API best practices","tags":[],"categories":[]},{"title":"Distributed Programming in PyTorch","date":"2021-11-30T13:22:10.000Z","path":"wiki/distributed-programming-in-pytorch/","text":"Distributed Programming in PyTorchPyTorch는 멀티 GPU 학습을 위해 torch.nn.DataParallel을 제공한다. 이 클래스의 사용법은 상당히 간단한데, 모델을 만든 후, torch.nn.DataParallel을 씌워주면 된다. 예를들어, 다음과 같다. 12345678910111213141516class Model(nn.Module): def __init__(self): # ... def forward(self, x): # ...model = Model().cuda()dmodel = nn.DataParallel(model).cuda()for e in range(EPOCHS): for x, y in trainloader: logps = dmodel(x) # 여기서, model 대신, dmodel을 사용하면 됨 # dmodel은 x를 GPU개수만큼 잘라서 각각 돌리고 # 그 결과를 concat해서 반환해줌 # loss 계산, acc 계산, back-prop 등등 처리 이처럼, Model이 들어가야 할 자리에 torch.nn.DataParallel 객체를 넣어주면 된다. 매우 간편하게 멀티 GPU를 사용할 수 있는 셈이다. 하지만, 이 torch.nn.DataParallel 은 치명적인 문제점이 존재한다. torch.nn.DataParallel은 Thread 기반이다. 아시는분들은 아시겠지만, python의 멀티스레드 프로그래밍에서는 GIL이라는 큰 제약조건이 존재한다. 💡 GIL(Global Interpreter Lock)이란, 파이썬 인터프리터의 구현 모델중 하나로, 파이썬 인터프리터의 동기화 문제 등으로 인해, 동시에 구동될 수 있는 파이썬 스레드 개수를 최대 1개로 제한해버린 구현 모델을 말한다. 즉, 파이썬 코드로 스레드를 많이 만들어서 코드를 구동하고, 컴퓨터에 프로세서가 많다고 해도, 파이썬은 한 번에 하나의 스레드만 돌린다. 따라서, torch.nn.DataParallel을 사용한다고 해서, GPU가 늘어나는 만큼 그 속도가 증가하지는 못한다. PyTorch에는 이러한 문제를 해결하기 위해 torch.nn.parallel.DistributedDataParallel 이라는 클래스를 제공한다. torch.nn.parallel.DistributedDataParallel은 스레드기반이 아닌 프로세스 기반이다. 파이썬에서는 threading이라는 모듈을 제공하지만, 말했다시피, GIL으로 인해 threading의 성능은 그렇게 좋지 못하다. 파이썬에서는 다행히도 GIL을 피할 수 있게 multiprocessing이라는 모듈을 standard library로 제공하고 있다. PyTorch에서는 이 파이썬 multiprocessing 모듈처럼, torch.multiprocessing 모듈과 torch.distributed 모듈을 제공하며, 이는 스레드 기반 분산처리보다 더 나은 분산처리를 구현하는 데 사용될 수 있다. 우리가 여기서 논의할 내용이 바로 PyTorch에서 제공하는 프로세스 기반 분산 프로그래밍이다. Distributed Programming in PyTorch여기서 논의할 내용은 다음과 같다. PyTorch에서의 프로세스 생성 방법 PyTorch의 프로세스 그룹과 init_process_group 함수 PyTorch 프로세스간의 blocking 통신 PyTorch 프로세스간의 non-blocking 통신 torch.nn.parallel.DistributedDataParallel의 경우, 여기서 다루지는 않고, 나중에 다른 문서에서 다루도록 할 것이다. 본 문서에서는 torch.nn.parallel.DistributedDataParallel를 공부하기 이전에 PyTorch에서 어떻게 프로세스 단위로 프로그래밍할 수 있는지 알아볼 것이다. Process Creation in PyTorchPyTorch에서는 torch.multiprocessing이라는 모듈을 제공하는데, 파이썬 standard library에서의 multiprocessing 모듈과 유사하게 생겼다. PyTorch에서 프로세스를 생성하는 방법은 다음과 같다. 1234567891011from torch.multiprocessing import Processdef func(args): # ...args = [] # func에 넘겨줄 인자 리스트p = Process(target=func, args=args) # 프로세스 생성. 이때, 아직, # 프로세스는 시작하지 않음(func을 구동하지 않음)p.start() # 여기서 비로소 프로세스 시작p.join() # 프로세스가 끝날때까지 기다리고 자원 회수 이 형태는 파이썬 threading 라이브러리 사용법과도 매우 유사한데, PyTorch에서 프로세스를 생성하는 방법은 Process 객체를 생성하는 것이다. 이때, Process 생성자로 넘겨주는 파라미터 중 가장 중요한 두 개의 인자는 바로 target과 args이다. target인자는 해당 프로세스가 생성되고 실행할 함수를 의미하며, args는 그 함수에 넘겨줄 파라미터를 받는다. 여기서, target은 func으로 설정되어 있기 때문에, 프로세스는 시작할 때, func 함수를 실행하게 되며, args라는 파라미터를 func의 파라미터로 넘겨주게 된다. Process 를 통해 프로세스를 생성한다고 해서 프로세스가 바로 시작하지는 않는다. Process 객체의 start 메소드를 호출해야 해당 프로세스가 비로소 target함수를 실행하게 된다. Linux 시스템 프로그래밍 경험이 있으시다면, join함수의 중요성을 잘 알 것이다. 기본적으로 프로세스를 코드상에서 생성하게 되면, 그 프로세스는 코드를 구동하는 프로세스의 자식 프로세스로 생성된다. 그리고, 이것은, 코드를 실행하는 프로세스가 자식 프로세스의 자원을 회수할 의무가 있다는 것을 의마한다. 코드상에서 fork, Process와 같은 코드를 사용하여 프로세스를 생성했다면, 코드에서 그 프로세스의 죽음을 지켜보고 그 자원을 회수할 의무가 있다. 그 역할을 하는 함수가 바로 join이다. 만약, 코드를 실행하는 프로세스가 자식을 회수하지 않고 끝나버리면, 그 자식 프로세스는 데몬 프로세스가 되는데, 물론, 요즘 운영체제는 부모가 죽으면, 그 자식의 자원을 자동으로 회수해주긴 하지만, 명시적으로 join을 호출해주는 습관을 들이는게 좋다. Process GroupPyTorch의 멀티프로세싱에서는 프로세스 그룹이라는 개념이 있다. 프로세스 그룹이란, 말 그대로 여러 프로세스를 묶어 관리하는 것을 말하는데, 프로세스 그룹 안에 속해있는 프로세스들은 peer 프로세스가 몇개가 있고 누가 있는지 알 수 있다. 즉, 그에따라, peer 프로세스와 통신이 가능하다. 프로세스 그룹은 처음볼때는 이해하기가 좀 까다로운데, 프로세스 그룹이 형성되는 과정은 다음과 같다. 루트프로세스에서 여러개의 자식 프로세스를 생성한다. 각 자식 프로세스에서, 마스터(프로세스 중 하나를 마스터로 삼는다)의 주소를 알 수 있게 하는 환경변수를 설정한다. MASTER_ADDR: 마스터의 ip주소 MASTER_PORT: 마스터의 포트번호 개발자는 각 자식 프로세스 코드에서 init_process_group 함수를 호출하여, 해당 프로세스가 프로세스 그룹에 속한다는 것을 알려주어야 한다. 즉, init_process_group은 해당 프로세스에게, 그룹에 소속되어있다는 사실을 알려주는 함수이다. init_process_group이 실행되면, 해당 프로세스는 MASTER_ADDR와 MASTER_PORT를 사용하여 마스터와 통신을 시도한다. 마스터는 여러 프로세스로부터 통신을 받게 되며, 마스터는 그 프로세스들을 서로 통신할 수 있도록 연결시켜준다. 여기서, 알아두어야 할 점은, init_process_group함수는 프로세스를 생성하지는 않는다. 프로세스 그룹을 생성하는 역할을 하는 것이다. 또한, 다른 peer 프로세스가 누가 있는지, 알 수 있게 해 주는 함수가 바로 init_process_group이며, 즉, 다른 peer 프로세스의 존재를 알려줌으로써, 나중에 peer 프로세스와 메시지를 주고받을 수 있게 해 주는 함수도 init_process_group이다. Example of init_process_group다음은 init_process_group를 사용하는 예제코드를 보여준다(Reference: PyTorch 공식홈피). 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import osimport torchimport torch.distributed as distfrom torch.multiprocessing import Processdef run(rank, size): &#x27;&#x27;&#x27; 프로세스가 실행하는 메인 함수 &#x27;&#x27;&#x27; # some logicdef init_process(rank, size, fn, backend=&quot;gloo&quot;): &#x27;&#x27;&#x27; 이 함수는 각 프로세스에서 실행되는 함수이다. 이 함수는 각 프로세스가 만들어졌을 때, 프로세스 그룹을 세팅하기 위한 함수로, 세팅이 끝나면, 마지막으로 메인 함수인 run을 호출한다. &#x27;&#x27;&#x27; print(f&quot;I am a &#123;rank&#125; process!&quot;) # 각 프로세스에서는 MASTER_ADDR와 MASTER_PORT 환경변수를 설정해준다. os.environ[&quot;MASTER_ADDR&quot;] = &quot;127.0.0.1&quot; os.environ[&quot;MASTER_PORT&quot;] = &quot;24500&quot; # 이 프로세스는 127.0.0.1:24500 을 마스터로 하는 프로세스 그룹에 속한다는 것을 # 알려준다. dist.init_process_group(backend, rank=rank, world_size=size) # fn 함수 호출 fn(rank, size)if __name__ == &quot;__main__&quot;: # GPU 개수 얻어오기 size = torch.cuda.device_count() print(&quot;Device count:&quot;, size) processes = [] # GPU개수만큼 프로세스 생성 후 각 프로세스에서 init_process 구동 for rank in range(size): p = Process(target=init_process, args=(rank, size, run)) p.start() processes.append(p) # 모든 자식이 죽을때까지 기다렸다가 자원 회수 for p in processes: p.join() 각 프로세스가 생성되면, init_process라는 함수가 호출되는데, init_process는 해당 프로세스의 환경변수를 세팅하고 프로세스 그룹에 포함된다는 사실을 알려주는(즉, init_process_group을 호출하는) 역할을 수행하는 함수이다. 이러한 세팅이 끝나면, 곧이어 run 함수를 호출하게 되고, run 함수에서 로직을 수행하게 되는 것이다. init_process_group함수를 자세히 살펴보자. init_process_group은 많은 파라미터를 받을 수 있지만, 이 예제에서는 backend, world_size, rank 세 가지의 파라미터만 사용한다. 다음은 init_process_group의 각 파라미터에 대한 설명이다. backend 파라미터 PyTorch에서 프로세스 그룹을 생성한다는 것은 여러 프로세스가 서로 통신하며 분산처리를 하겠다는 것을 의미한다. 따라서, 분산처리 백엔드 라이브러리를 사용해야 하며, backend 파라미터는 어떤 백엔드를 사용하여 분산처리를 할 것인지 명시하도록 한다. PyTorch는 scratch부터 low-level부터 high-level까지의 분산처리 시스템을 구현한 것이 아니라, 다른 low-level 분산처리 라이브러리를 가저다 쓰는 형태이다. 즉, 백엔드는 다른 라이브러리를 사용한다. PyTorch에서는 세 가지의 백엔드를 지원한다. GLOO: PyTorch에서 가장 잘 지원하는 라이브러리로, CPU 텐서와 GPU 텐서의 분산처리 모두를 지원한다. 다만, GPU 텐서 연산의 경우 NCCL 백엔드보다는 최적화가 덜 되어 있다고 한다. MPI: Messaging passing interface를 사용하는 백엔드. 이것을 백엔드로 쓰려면, MPI 라이브러리의 추가적인 설치와 세팅이 필요하다. 자세한것은 여기 참고 NCCL: GPU 텐서연산에 대해 고도로 최적화된 백엔드이다. GPU에 올라간 텐서만 통신을 지원한다. world_size 파라미터 해당 프로세스 그룹에 속할 프로세스의 총 개수를 의미하며, 각 프로세스에게 몇 개의 peer 프로세스가 있는지 알 수 있도록 해 주는 인자이다. 프로세스가 마스터와 통신할때, 몇 개의 peer 프로세스가 접속하기를 기다려야 하는지 알려주는 인자이기도 하다. 즉, 프로세스가 init_process_group을 호출하게 되면, 마스터와 통신을 시도하고, 마스터는 그 프로세스에게 다른 peer 프로세스를 연결시켜 주는데, world_size 파라미터를 명시해주게 되면, 몇 개의 peer 프로세스가 자신과 연결되어야 하는지 알 수 있게 된다(world_size - 1개의 프로세스가 자신과 연결되어야 함). 만약, 마스터가 자신에게 world_size - 1개의 프로세스를 아직 연결해주지 못했다면(다른 peer 프로세스가 아직 init_process_group을 호출하지 못했다거나의 이유로) 해당 프로세스는 마스터와 계속 연결을 유지하면서 peer를 기다린다. rank 파라미터 해당 프로세스의 id값이라고 보면 된다. 프로세스 그룹이 생성될때, 각 프로세스는 rank의 값으로 자신의 id를 설정한다. 이 값은 나중에 다른 프로세스와 통신할때 사용된다. 즉, 3번 rank를 가진 프로세스에게 tensor를 전송하라! 라는 명령이 가능하다. os.environ[&quot;MASTER_ADDR&quot;]와 os.environ[&quot;MASTER_PORT&quot;]의 설정, 그리고, init_process_group은 각각의 프로세스에서 모두 한번씩 실행되어야 함을 주의해야 한다. Blocking Communications in PyTorchinit_process_group을 이용하여 각각의 프로세스에게 peer 프로세스가 누가 있는지 알려줄 수 있었다. init_process_group을 통해 peer 프로세스가 누군지 알 수 있게 되면, peer 프로세스의 rank 번호를 이용하여 peer 프로세스와 통신이 가능하다. 통신에는 blocking 통신과 non-blocking 통신이 존재하는데, 먼저, blocking 통신에 대해 논의해보도록 한다. PyTorch에서의 다른 프로세스와의 통신은, 텐서를 주고받는 것이다. 텐서를 보내는 함수로는 torch.distributed.send가 있고, 텐서를 받는 함수로는 torch.distributed.recv가 있다. torch.distributed.send를 호출하게 되면, 상대방 프로세스가 recv를 호출하여 받을 때까지 blocking된다. 마찬가지로, torch.distributed.recv를 호출하게 되면, 상대방 프로세스가 send를 통해 무언가를 보낼때 까지 blocking된다. 따라서, 타이밍을 잘 맞춰서 사용하도록 하자. Example of Blocking Communication in PyTorch다음은 init_process_group의 예제에다가 blocking communication을 구현한 예제이다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import os# GPU개수는 2개라고 가정하자os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0,1&quot;import torchimport torch.distributed as distfrom torch.multiprocessing import Processdef run(rank, size): &#x27;&#x27;&#x27; 프로세스가 실행하는 메인 함수 &#x27;&#x27;&#x27; tensor = torch.FloatTensor([0.0]) if rank == 1: tensor += 1 dist.send(tensor=tensor, dst=0) elif rank == 0: dist.recv(tensor=tensor, src=1) print(&quot;Rank:&quot;, rank, &quot;, Tensor:&quot;, tensor[0])def init_process(rank, size, fn, backend=&quot;gloo&quot;): &#x27;&#x27;&#x27; 이 함수는 각 프로세스에서 실행되는 함수이다. 이 함수는 각 프로세스가 만들어졌을 때, 프로세스 그룹을 세팅하기 위한 함수로, 세팅이 끝나면, 마지막으로 메인 함수인 run을 호출한다. &#x27;&#x27;&#x27; print(f&quot;I am a &#123;rank&#125; process!&quot;) # 각 프로세스에서는 MASTER_ADDR와 MASTER_PORT 환경변수를 설정해준다. os.environ[&quot;MASTER_ADDR&quot;] = &quot;127.0.0.1&quot; os.environ[&quot;MASTER_PORT&quot;] = &quot;24500&quot; # 이 프로세스는 127.0.0.1:24500 을 마스터로 하는 프로세스 그룹에 속한다는 것을 # 알려준다. dist.init_process_group(backend, rank=rank, world_size=size) # fn 함수 호출 fn(rank, size)if __name__ == &quot;__main__&quot;: # GPU 개수 얻어오기 size = torch.cuda.device_count() print(&quot;Device count:&quot;, size) processes = [] # GPU개수만큼 프로세스 생성 후 각 프로세스에서 init_process 구동 for rank in range(size): p = Process(target=init_process, args=(rank, size, run)) p.start() processes.append(p) # 모든 자식이 죽을때까지 기다렸다가 자원 회수 for p in processes: p.join() 프로세스의 생성과 init_process_group까지는 이전 예제와 같다. 달라진점은 run 함수 내에 다른 peer 프로세스와 통신하는 코드를 넣었다는 점이다. 말했다시피, 통신은 send함수와 recv함수를 사용한다. send함수는 tensor 객체를 dst 프로세스에게 보내는데, dst에 들어갈 값은 메시지를 받을 프로세스의 rank값이다. recv함수 역시, 비슷하게 사용하면 되는데, src 프로세스로부터 어떤 텐서를 받아서 tensor 객체에 저장한다. 이때, tensor는 미리 생성해두어야 하며, recv는 받은 텐서객체를 tensor에 덮어씌우게 된다. 실행결과, 텐서값은 두 프로세스 모두 1이 된다. 0번 프로세스의 경우, 직접 tensor += 1을 해주었고, 1번 프로세스의 경우, 0번으로부터 받은 텐서로 덮어씌웠기 때문이다. Non-blocking Communication in PyTorchPyTorch에서 process간 통신은 blocking과 non-blocking방식 모두 존재한다고 말했었다. PyTorch는 torch.distributed.send와 torch.distributed.recv라는 blocking 통신을 제공했는데, 이와 비슷하게 torch.distributed.isend, torch.distributed.irecv라는 non-blocking 통신을 제공한다. torch.distributed.isend를 사용하여 텐서를 그룹 내 어떤 프로세스에게 전송하게 되면, 그 텐서를 수신자가 받을 때까지 기다리지 않고, 바로 리턴되는데, 이때, torch.distributed.isend는 request 객체를 반환한다(torch.distributed.send는 아무것도 반환하지 않음). 메시지가 전달되었는지에 대한 여부는 request 객체의 wait 메소드를 호출하면 알 수 있다. wait 메소드를 호출했을 때, blocking되면 아직 전송이 안된 것이고, 리턴되면 전송된 것이다. 마찬가지로, torch.distributed.irecv를 호출하게 되면, 텐서가 올때까지 기다리는 게 아니라 request객체를 바로 반환하게 된다(백그라운드에서 송신자로부터 텐서를 계속 기다리고 있음). 텐서가 왔는지 확인하려면 request 객체의 wait 메소드를 호출하면 된다. 역시, blocking되면, 아직 텐서를 못받은 것이고, 리턴되면 텐서를 받은 것이다. Example of Non-blocking Communication in PyTorch12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import torchimport torch.distributed as distfrom torch.multiprocessing import Processdef run(rank, world_size): &#x27;&#x27;&#x27; 프로세스가 실행하는 메인 함수 &#x27;&#x27;&#x27; tensor = torch.FloatTensor([0.0]) if rank == 0: tensor += 1 req = dist.isend(tensor=tensor, dst=1) else: req = dist.irecv(tensor=tensor, src=0) req.wait() print(&quot;Rank:&quot;, rank, &quot;, Tensor:&quot;, tensor[0])def init_process(rank, world_size, fn, backend=&quot;gloo&quot;): &#x27;&#x27;&#x27; 이 함수는 각 프로세스에서 실행되는 함수이다. 이 함수는 각 프로세스가 만들어졌을 때, 프로세스 그룹을 세팅하기 위한 함수로, 세팅이 끝나면, 마지막으로 메인 함수인 run을 호출한다. &#x27;&#x27;&#x27; print(f&quot;I am a &#123;rank&#125; process!&quot;) # 각 프로세스에서는 MASTER_ADDR와 MASTER_PORT 환경변수를 설정해준다. os.environ[&quot;MASTER_ADDR&quot;] = &quot;127.0.0.1&quot; os.environ[&quot;MASTER_PORT&quot;] = &quot;24500&quot; # 이 프로세스는 127.0.0.1:24500 을 마스터로 하는 프로세스 그룹에 속한다는 것을 # 알려준다. dist.init_process_group(backend, rank=rank, world_size=world_size) # fn 함수 호출 fn(rank, world_size)if __name__ == &quot;__main__&quot;: # GPU 개수 얻어오기 world_size = torch.cuda.device_count() processes = [] # GPU개수만큼 프로세스 생성 후 각 프로세스에서 init_process 구동 for rank in range(world_size): p = Process(target=init_process, args=(rank, world_size, run)) p.start() processes.append(p) # 모든 자식이 죽을때까지 기다렸다가 자원 회수 for p in processes: p.join() Reduce &amp; Broadcastdist.recv, dist.send, dist.irecv, dist.isend 처럼 텐서 단위를 직접 주고받는 연산도 있지만, 각 프로세스에 있는 텐서를 자동으로 모아서 합쳐준다거나, 하나의 텐서를 모든 프로세스에게 자동으로 뿌려주는 연산도 있다. 전자는 reduce, 후자는 broadcast로써 PyTorch에서 함수로 지원한다. ReduceReduce는 각 프로세스에 있는 텐서를 모아서 aggregation해주는 함수로, dist.reduce로 존재한다. 다음 예시 코드를 보자. 123456789101112131415161718192021222324252627282930313233import torchfrom torch.utils.data.distributed import DistributedSamplerfrom torch import distributed as distfrom torch import multiprocessing as mpimport timeimport osWORLD_SIZE = 4def setup(rank, world_size): os.environ[&quot;MASTER_ADDR&quot;] = &quot;127.0.0.1&quot; os.environ[&quot;MASTER_PORT&quot;] = &quot;25555&quot; os.environ[&quot;RANK&quot;] = f&quot;&#123;rank&#125;&quot; os.environ[&quot;WORLD_SIZE&quot;] = f&quot;&#123;world_size&#125;&quot; dist.init_process_group(&quot;gloo&quot;, rank=rank, world_size=world_size)def run(rank, world_size): setup(rank, world_size) while True: tensor = torch.randn(2, 2) # 4개의 머신에서 tensor를 모두 0번 머신으로 보내고, # 0번 머신은 tensor를 모두 모아서 SUM한 값을 tensor에 대입한다. dist.reduce(tensor, 0, dist.ReduceOp.SUM) print(tensor) time.sleep(1) if __name__ == &quot;__main__&quot;: mp.spawn(run, args=(WORLD_SIZE,), nprocs=WORLD_SIZE, join=True) 이 코드는 1초마다 4개의 프로세스에서 (2, 2) 크기의 랜덤 텐서를 생성하고 텐서를 0번 텐서에 모두 모아서 덧셈하는 것을 수행하는 코드이다. 각 머신이 dist.reduce(tensor, 0, dist.ReduceOp.SUM)라는 코드를 만났을 때 취하는 행동은 다음과 같다. 자신이 0번 프로세스인 경우 다른 프로세스로부터 값을 모으고 모두 SUM해서 tensor에 저장한다. 자신이 0번 프로세스가 아닌 경우 0번 프로세스에게 tensor값을 전송한다. 위 코드에서, 0번 머신은 tensor값이 변하지만, 나머지 프로세스는 tensor값이 변하지 않는다. 0번 머신은 다른 머신이 보낸 텐서를 모두 모아서 더하고 그 값을 tensor로 저장했지만, 나머지 머신은 그저 0번 머신에게 전송만 했기 때문이다. BroadcastBroadcast는 PyTorch에서 dist.boardcast로 제공되는 기능으로, 하나의 프로세스에서 다른 프로세스로 텐서를 뿌려주는(전파시키는) 역할을 하는 함수이다. 다음 예제를 보자. 12345678910from torch import distributed as distdef average_gradient(model, world_size): for param in model.parameters(): # gradient를 0번 머신이 모아서 평균냄 dist.reduce(param.grad.data, 0, dist.ReduceOp.SUM) param.grad.data /= world_size # 0번 머신은 gradient를 모두에게 전송함 dist.broadcast(param.grad.data, 0) 이 예제에서, dist.broadcast는 0번 머신이 자신의 데이터(gradient)를 모든 머신에게 전송하는 역할을 한다. 각 프로세스가 dist.broadcast(tensor, 0)라는 코드를 만났을때, 수행하는 행동은 다음과 같다. 자신이 0번 프로세스인 경우 자신의 tensor데이터를 모두에게 전송한다. 자신이 0번 프로세스가 아닌 경우, 0번 프로세스로부터 데이터를 받을때까지 기다리고, 데이터를 받으면 tensor에 저장한다. Machine-to-Machine CommunicationPyTorch 통신은 프로세스끼리도 가능하지만 다른 컴퓨터끼리도 가능하다. 해답은 MASTER_ADDR를 다른 컴퓨터 IP 주소로 설정하는 것이다. 1234567def setup(rank, world_size): os.environ[&quot;MASTER_ADDR&quot;] = &quot;???.???.???.???&quot; os.environ[&quot;MASTER_PORT&quot;] = &quot;25555&quot; os.environ[&quot;RANK&quot;] = f&quot;&#123;rank&#125;&quot; os.environ[&quot;WORLD_SIZE&quot;] = f&quot;&#123;world_size&#125;&quot; dist.init_process_group(&quot;nccl&quot;, rank=rank, world_size=world_size) MASTER_ADDR에 들어가는 주소는 다른 컴퓨터가 인지 가능한 주소여야 하며, 마스터 자신또한 자신이 해당 주소라는 것을 알 수 있어야 한다. “마스터 자신또한 자신이 해당 주소라는 것을 알 수 있어야 한다”는 점도 상당히 중요한데, 만약, GCP(Google Cloud Platform)와 같은 서비스를 이용해서 분산 처리 시스템을 구현한다면, 이게 문제가 될 수 있다. 만약, GCP의 같은 계정, 같은 region 안에 존재하는 서버끼리는(또는 같은 VPC 네트워크 내에 존재하는 서버끼리는) 문제없이 통신되지만, 다른 GCP 계정에 존재하는 서버와 PyTorch 통신을 하려는 경우, 이게 문제가 생긴다. dist.init_process_group에서는 외부 IP로 연결이 가능하지만, 그 이후에 각 머신이 서로 통신하는 과정은 각 머신이 알고 있는 자신의 IP로 통신을 시도한다. 근데, GCP VM은 자신의 외부 IP를 모른다(ifconfig 입력해보면 자신의 외부 IP가 나오지 않음). 이것 때문에, dist.init_proces_group은 통과하지만, 그 이후 통신에서 connect가 안되는 문제가 발생할 수 있다. 만약, 본인 계정에서 머신 여러개를 만들고 통신하는건 문제가 되지 않는다. 하지만, 다른 VPC 네트워크에 존재하는 VM으로의 통신은 필자가 알기로는 방법이 없어 보인다. 따라서, 다른 VPC 네트워크에 속한 VM과 통신하고자 한다면, PyTorch 통신 이외에 다른 통신법을 찾아보기를 권한다.","tags":[{"name":"프로그래밍","slug":"프로그래밍","permalink":"https://the-masked-developer.github.io/tags/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"}],"categories":[{"name":"프로그래밍","slug":"프로그래밍","permalink":"https://the-masked-developer.github.io/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"}]},{"title":"Helm 사용해보기","date":"2021-11-20T14:15:22.000Z","path":"wiki/helm/","text":"Helm 사용해보기 1. Helm이란. 비유를 한다면 개발자가 Docker Registry에서 검색하여 쉽게 이미 셋팅된 컨테이너 환경을 실행하듯이Helm에서 검색하여 이미 셋팅된 쿠버네티스 환경을 실행한다라고 이해하면 되지 않을 듯 싶다.ex. Dockerhub처럼 ArtifactHub에서 많은 템플릿이 등록되고 공유되고 있다. 컨테이너 환경을 Dockerfile로 구성 하는 것처럼 Helm도 차트로 구성을 한다. 차트 = 쿠버네티스 YAML을 템플릿으로 구성한 파일들 2. Usage. Helm Chart 생성 Helm Chart를 생성하기 위해서는 위에서 설명한 구조의 파일들을 생성할 필요가 있음 nginx 템플릿을 단계별로 구성하여 따라해보기 2-1. /templates 폴더 생성하여 배포할 쿠버네티스 YAML 작성 deployment, service YAML은 뒤에 작성 1234.└── templates ├── deployment.yaml └── service.yaml deployment.yaml123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-test labels: app: nginx-testspec: replicas: 1 selector: matchLabels: app: nginx-test template: metadata: labels: app: nginx-test spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 service.yaml1234567891011apiVersion: v1kind: Servicemetadata: name: nginx-testspec: selector: app: nginx-test ports: - port: 80 targetPort: 80 type: ClusterIP 2-2. Chart.yaml 작성 Chart.yaml = Helm 차트의 이름, 버전 등 메타데이터를 입력하는 파일 123apiVersion: v2 # 차트 API 버전 (필수)name: nginx-test # 차트명 (필수)version: 0.0.1 # SemVer 2 버전 (필수) 현재 파일 구성 12345.├── Chart.yaml└── templates ├── deployment.yaml └── service.yaml 2-3. values.yaml 작성 values.yaml = /templates 내부에 쿠버네티스 YAML에 있는 값을 동적으로 지정할 때 사용하는 파일 일단은 파일이 비어있는 상태로도 설치가 가능하기 때문에 파일만 생성 1touch values.yaml 123456.├── Chart.yaml├── templates│ ├── deployment.yaml│ └── service.yaml└── values.yaml 2-4. Helm Chart 설치1helm install &lt;이름&gt; &lt;차트_경로&gt; -n &lt;네임스페이스&gt; 실행 결과 12345678# helm install nginx-test . -n testNAME: nginx-testLAST DEPLOYED: Sun Nov 21 05:41:08 2021NAMESPACE: testSTATUS: deployedREVISION: 1 # helm upgrade 반영 횟수TEST SUITE: None Helm 조회 123# helm ls -n testNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSIONnginx-test test 1 2021-11-21 05:41:08.564944731 +0900 KST deployed nginx-test-0.0.1 2-5. 실제 쿠버네티스 배포현황 확인123456# k get deploy,svc -n testNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/nginx-test 1/1 1 1 4m3sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/nginx-test ClusterIP 10.100.29.142 &lt;none&gt; 80/TCP 4m3s 3. Usage. Helm Chart 삭제1helm delete &lt;릴리즈된_차트명&gt; -n &lt;네임스페이스&gt; 실행 결과 123# helm delete nginx-test -n testrelease &quot;nginx-test&quot; uninstalled 이후, helm ls -n test 명령어 실행시 차트가 삭제된 것을 확인할 수 있음 4. Usage. values.yaml으로 템플릿 문법 사용 “values.yaml = /templates 내부에 쿠버네티스 YAML에 있는 값을 동적으로 지정할 때 사용하는 파일” 라고 이야기된 단계에 관한 설명 위 과정을 하기 위해서는 템플릿 문법을 사용해야함 예시로 /templates 내에 deployment.yaml에서 이미지 버전이 현재 latest로 되어있는 것을 외부에서 바꿀 수 있게 변경을 진행 4-1. /templates/*.yaml 수정1234567... spec: containers: - name: nginx image: nginx:&#123;&#123; .Values.version &#125;&#125; ports: - containerPort: 80 4-2. values.yaml 수정1version: stable 3. Helm Chart 설치1helm install nginx-values-test . 실행 결과 123456NAME: nginx-values-testLAST DEPLOYED: Sun Nov 21 06:11:26 2021NAMESPACE: testSTATUS: deployedREVISION: 1TEST SUITE: None 배포 확인: 아래처럼 image version tag가 바뀐 것을 확인할 수 있다. 1k get deploy nginx-test -o yaml 12... - image: nginx:stable Helm Chart로 공개된 오픈소스 문법 참고 Nginx Prometheus Grafana 5. Usage. Release 템플릿 문법 사용기존 .Values 템플릿 문법으로는 helm install &lt;차트명&gt;에서 입력되는 차트명까지는 가져오지 못한다.이러한 문제를 해결해주기 위한 템플릿 문법으로 .Release가 있다. 1234567891011121314151617# 1. /templates/deployment.yaml 릴리즈 템플릿 적용apiVersion: apps/v1kind: Deploymentmetadata: name: &#123;&#123; .Release.Name &#125;&#125; labels: app: &#123;&#123; .Release.Name &#125;&#125;spec: replicas: 1 selector: matchLabels: app: &#123;&#123; .Release.Name &#125;&#125; template: metadata: labels: app: &#123;&#123; .Release.Name &#125;&#125;... 123456789101112# 2. /templates/service.yaml 릴리즈 템플릿 적용apiVersion: v1kind: Servicemetadata: name: &#123;&#123; .Release.Name &#125;&#125;spec: selector: app: &#123;&#123; .Release.Name &#125;&#125; ports: - port: 80 targetPort: 80 type: ClusterIP Helm Chart 설치 (.Release 문법 적용) 1helm install nginx-rename-test . -n test 실행 결과 1234567891011# helm install nginx-rename-test . -n testNAME: nginx-rename-testLAST DEPLOYED: Sun Nov 21 06:48:04 2021NAMESPACE: testSTATUS: deployedREVISION: 1TEST SUITE: None# helm ls -n testNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSIONnginx-rename-test test 1 2021-11-21 06:48:04.719317331 +0900 KST deployed nginx-test-0.0.1 nginx-values-test test 1 2021-11-21 06:11:26.928779655 +0900 KST deployed nginx-test-0.0.1 아래 명령을 실행하면, Helm이 최종적으로 어떤 정보로 릴리즈를 했는지 알 수 있다. Helm 명명된 릴리스에 대한 모든 정보 조회 1helm get all &lt;RELEASE_NAME&gt; 실행 결과 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# helm get all nginx-rename-testNAME: nginx-rename-testLAST DEPLOYED: Sun Nov 21 06:48:04 2021NAMESPACE: testSTATUS: deployedREVISION: 1TEST SUITE: NoneUSER-SUPPLIED VALUES:nullCOMPUTED VALUES:version: stableHOOKS:MANIFEST:---# Source: nginx-test/templates/service.yamlapiVersion: v1kind: Servicemetadata: name: nginx-rename-testspec: selector: app: nginx-rename-test ports: - port: 80 targetPort: 80 type: ClusterIP---# Source: nginx-test/templates/deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: nginx-rename-test labels: app: nginx-rename-testspec: replicas: 1 selector: matchLabels: app: nginx-rename-test template: metadata: labels: app: nginx-rename-test spec: containers: - name: nginx image: nginx:stable ports: - containerPort: 80 6. Usage. 그 외 Helm 자주 사용하는 명령어 릴리스 히스토리를 가져오기, 업그레이드 내역(revision) 확인 등 유용 1helm history &lt;릴리즈_차트명&gt; Helm Chart 문법 검사 1helm lint &lt;차트_경로&gt; Helm /templates + values.yaml 최종 결과를 보여준다. 1helm template &lt;차트명&gt; &lt;차트_경로&gt; Helm 차트 릴리즈 업그레이드 추후 과정에서 더 자세히 설명 1helm upgrade &lt;릴리즈_차트명&gt; &lt;차트_경로&gt; 7. Usage. values.yaml override 보통의 경우에는 values.yaml을 직접 작성하지 않고, 남이 작성한 파일을 가져오기 때문에 override(덮어쓰기)하여 사용이 된다. 아래의 2가지 방법이 있다. --set 인자 사용 -f 옵션으로 파일을 작성 1. --set 인자 사용1helm install --set version=1.21.4 -n test nginx-set-test . 실행 결과 123456NAME: nginx-set-testLAST DEPLOYED: Sun Nov 21 07:09:41 2021NAMESPACE: testSTATUS: deployedREVISION: 1TEST SUITE: None 배포 결과 (values.yaml version 값이 stable이지만 1.21.4로 된 것을 알 수 있음) 123# k get deploy -n test nginx-set-test -o yaml | grep image... - image: nginx:1.21.4 2. -f 옵션으로 파일을 작성1helm install -f &lt;파일경로&gt; -n test nginx-file-test . 2-1. override_values.yaml 파일을 아래와 같이 작성1version: stable-alpine 차트 파일 구성 123456789.├── Chart.yaml├── override_values.yaml # 추가됨├── templates│ ├── deployment.yaml│ └── service.yaml└── values.yaml1 directory, 5 files 2-2. -f 옵션으로 Helm Chart 설치1helm install -f override_values.yaml -n test nginx-file-test . 실행 결과 123456NAME: nginx-file-testLAST DEPLOYED: Sun Nov 21 07:16:31 2021NAMESPACE: testSTATUS: deployedREVISION: 1TEST SUITE: None 배포 결과 123# k get deploy -n test nginx-file-test -o yaml | grep image... - image: nginx:stable-alpine 실제 override 할 때는 각 차트의 configuration 항목을 참조하여 변경 필요한 값만 지정하여 설치 예제. nginx chart 8. Usage. Helm Chart 업그레이드 기존 릴리즈된 차트 내용을 변경 요구시 사용 이력 관리를 위해 사용 1. Helm Chart 기본 설치1helm install -n test nginx-test . 결과 확인 (revision)1helm get all nginx-test 2. Helm Chart 업그레이드1helm upgrade --set version=stable-alpine -n test nginx-test . REVISION이 올라가고 이미지 배포 버전이 바뀐 것을 알 수 있음 values.yaml을 변경하고 업그레이드를 진행하여도 변경이 됨 업그레이드시 주의사항 Helm 업그레이드시 Pod가 재기동됨(RollingUpdate) 12345# 1helm upgrade --set version=stable-alpine -n test nginx-test .# 2helm upgrade -n test nginx-test . 위 과정을 진행하면, helm 버전에 따라 values.yaml참고가 다르다. version3: 이전 revision을 그대로 따라 버전을 유지 version2: 이전 revision 버전을 유지하지 않는다. (첫 설치 values.yaml) 9. Helm Rollback1helm rollback &lt;릴리즈_차트명&gt; &lt;REVISION&gt; 실행 결과 12345678910[root@acc-master nginx-chart]# helm rollback -n test nginx-test 1Rollback was a success! Happy Helming![root@acc-master nginx-chart]# helm lsNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSIONnginx-test test 3 2021-11-21 07:47:41.211560477 +0900 KST deployed nginx-test-0.0.1 [root@acc-master nginx-chart]# k get deploy -n test nginx-test -o yaml | grep image f:image: &#123;&#125; f:imagePullPolicy: &#123;&#125; - image: nginx:stable imagePullPolicy: IfNotPresent 롤백은 한다고 해도 revision은 이력관리를 위해 증가한다. 버전이 최초 설치와 동일한 stable로 변경된 것을 알 수 있다.","tags":[{"name":"Helm","slug":"Helm","permalink":"https://the-masked-developer.github.io/tags/Helm/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://the-masked-developer.github.io/tags/Kubernetes/"}],"categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://the-masked-developer.github.io/categories/Kubernetes/"},{"name":"Helm","slug":"Kubernetes/Helm","permalink":"https://the-masked-developer.github.io/categories/Kubernetes/Helm/"}]},{"title":"Bayesian Statistics 01. Probability","date":"2021-11-16T14:04:10.000Z","path":"wiki/bayesian-01-probability/","text":"01. Probability우리는 세상을 살아가는 데 있어서, 무수히 많은 결정(decision)을 내리게 된다. 결정을 내릴 때, 우리는 해당 결정에 영향을 미친 원인들과 해당 결정을 내림으로써 미치는 결과 요인들을 고려하면서 결정을 내리게 된다. 하지만, 많은 요인들을 고려하면서 내린 결정이 잘못된 결정이 되는 경우도 많이 있는데, 이는 결정을 내릴 때 고려하는 요인들을 우리가 완벽히 알지 못하기 때문이다. 만약, 우리가 세상에 대해 100% 알고, 모든 인과관계를 파악할 수 있다면, 항상 옳은 결정을 내릴 수 있다. 하지만, 세상에는 우리가 알지 못하는 요인들이 무수히 많기 때문에, 우리는 최대한 많은 요인들을 살아오면서 습득한 경험과 지식으로 예측하고 판단하며 최종 결정을 내린다. 다시 말하지만, 이것은 우리가 세상에 존재하는 요인들을 완벽하게는 알지 못하기 때문이다. 불확실성이란, 어떤 요인에 대해 완전히 알지 못하기 때문에 발생하는 것으로, 해당 요인에 대해 미리 알고 있는 사전지식과 경험으로 판단하게 된다. 우리가 어떤 결정을 내릴 때, 우리는 결정에 영향을 미치거나 받는 모든 요인들의 불확실성을 고려하게 된다. 하지만, 이러한 불확실성을 모두 고려하기엔 인간의 능력은 한계가 있다. 확률은 불확실성을 수치화(정량화)함으로써, 불확실성을 수학으로 다룰 수 있도록 도와주는 도구이다. 확률을 이용하면 불확실성이라는 것을 정량화할 수 있고, 여러가지 수학 도구를 이용해서 불확실성을 모델링할 수 있다. 이러한 수학적 확률 모델은 여러가지 요인들에 대한 불확실성 속에서 우리가 최적의 결정을 내릴 수 있게 도와주는 도구이다. Background확률을 살펴보기에 앞서서, 필요한 용어들을 먼저 정리할 필요가 있다. 용어들의 정의를 살펴볼때, 집합이라는 개념을 유의하면서 살펴보면 좋다. Experiments 시행이라고도 불리며, sample space로부터 하나의 데이터 샘플을 얻는 행위를 말한다. Sample space Sample space란, 어떤 시행에 의해 나올 수 있는 모든 경우의 집합을 의미한다. 예를들어, 동전던지기라는 시행에서는 뒷면 또는 앞면만이 나올 수 있다. 이때, 동전던지기라는 experiment의 sample space는 ${\\text{HEAD}, \\text{TAIL}}$이 된다. 주사위를 던지는 experiment에 대해서의 sample space는 ${1, 2, 3, 4, 5, 6}$이 될 것이다. Events 사건이라고도 불리며, sample space의 부분집합이다. 예를 들어, 주사위를 던지는 시행에서, sample space는 ${1, 2, 3, 4, 5, 6}$이다. 그리고, 다음과 같은 event $A$를 정의할 수 있다. $A$: 짝수가 나오는 경우 이때, $A$는 sample space의 부분집합인 ${2, 4, 6}$이 된다. Random variables 확률변수라고도 불리며, 어떤 experiment를 통해 얻을 수 있는 outcome을 어떤 다른 라벨로 매핑하는 함수를 의미한다. 예를들어, 주사위를 던지는 experiment가 있다고 가정해보자. 이때, sample space는 ${1, 2, 3, 4, 5, 6}$이 된다. 이때, 우리는 random variable $X$를 다음과 같이 정의할 수 있다. $X = x_1$: 주사위가 짝수인 경우 $X=x_2$: 주사위가 홀수인 경우 $X=x_3$: 주사위가 4보다 크거나 같은 경우 Random variable $X$은 여러개의 이벤트중 하나의 이벤트를 취할 수 있으며, 주사위 던지기라는 experiment를 통해 얻을 수 있는 outcome들을 $x_1, x_2, x_3$ 중 하나로 매핑하게 된다. 이때, 이random variable $X$의 sample space는 ${x_1, x_2, x_3}$이 된다. 만약, 주사위를 던지는 시행을 통해 sample 4를 얻었다면, event $x_1$와 $x_3$가 동시에 일어난 것이다. Definition of Probability확률은 불확실성을 정량화하는 도구로, 불확실성을 수학으로 계산할 수 있게 하는 도구이다. 어떤 experiment(시행)에 대한 random variable(확률변수) $X$가 있고, $X$는 $x_1,…,x_k$의 event(사건, 경우)를 취할 수 있을 때, $x_i$가 일어날 확률은 대문자 $P$를 이용하여 $P(X=x_i)$로 정의한다. 또는 $p_X(X=x_i)$로 표기하거나 간단하게 $p(x_i)$로 표기하기도 한다(세 가지 표현법 모두 같은 의미임). 어떤 사건이 일어날 확률은 항상 0보다 크거나 같으며 1보다 작거나 같다. $$0 \\leq P(X=x_i) \\leq 1$$ How to Define Probability확률은 불확실성을 정량화해주는 도구이다. 불확실성을 정량화할때, 확률을 어떻게 정할지는 매우 중요한 문제이다. 불확실성을 수치로 표현하기 위한 확률은 다음과 같으 세 가지로 정의할 수 있다. Classical method Frequentist method Bayesian method Classical MethodEqually Likely Probability Sample space에서 모든 event들은 일어날 확률이 같다고 정의하는 방법이다. 동전 1번 던지는 시행에서 sample space는 앞면, 뒷면만 있다고 가정한다. 그럼 앞면이 나올 확률은 0.5이고, 뒷면이 나올 확률 역시 0.5이라고 정의한다. 하지만, 이러한 정의에는 문제가 있는데, 내일 날씨가 비가오거나, 맑거나, 우박이 내리는 3가지 경우만 있다고 가정해보자. 이때, classical method에 따르면, 맑을 확률은 0.33, 비가 올 확률도 0.33, 우박이 내릴 확률도 0.33이 된다. 따라서, classical method 방법은 매우 조심스럽게 사용해야 한다. Probability in Frequentist StatisticsRelative Rates of Events in Infinite Sequence 어떤 event의 확률을 정의할 때, “수많은 시행 가운데 그 event가 일어난 비율”이라고 정의하는 방법으로, Frequentist statistics에서 확률을 정의하는 방법이다. 즉, 데이터를 기반으로 확률을 정의하는 방법이다. 예를들어, 동전 던지기에서 앞면이 나올 확률을 계산하고 싶다면, 일단 동전을 무수히 많이 던져본다. 1000번을 던진 후, 651번의 앞면이 나왔다면, 동전 던지기 시행에서 앞면이 나올 확률은 0.651 로 정의하는 것이 Frequentist statistics 에서의 확률 정의이다. 이 경우, 위 classical method의 문제점을 설명할때 예로 들었던 날씨 예제의 문제는 해결이 된다. 1년동안 우박이 내리는 날은 매우 적기 때문에 우박이 내릴 확률은 매우 작게 측정될 것이다. 하지만, 이러한 정의에도 문제점이 있다. 어떤 이벤트가 일어날 확률을 계산하기 위해서는 많은 수의 샘플이 필요하고, experiment를 많이 수행해야 한다. 하지만, experiment가 가능한 경우는 실제로 그렇게 많지가 않다. 예를들어, 내일 비가 올 확률을 구하고 싶다고 해 보자. Frequentist statistics에 따르면, 내일 날씨를 여러번 샘플링 해야 한다. 즉, 내일 날씨를 확인하고, 다시 오늘로 돌아온 후 내일이 되면 날씨를 확인하고, 다시 오늘로 돌아와서 내일이 되면 날씨를 확인하고를 반복해야 한다. 하지만, 알다시피, 이건 타임머신이 있어야 가능하다. Probability in Bayesian StatisticsPersonal Perspective Bayesian statistics에서 확률을 정의하는 방법으로, 데이터와 함께 event에 대한 개인의 사전 지식을 바탕으로 확률을 정의한다. 예를 들어, 내일 비가 올 확률을 구할때, 자신이 가지고 있는 데이터 뿐 아니라, 오늘의 날씨를 보고 내일 비가 올 확률이 0.7정도 되겠구나 하는 개인의 믿음을 확률에 반영하게 된다. Bayesian statistics에서의 확률의 정의의 특징은, 개인의 사전 지식이 중요하게 작용하며, 많은 양의 데이터를 모으지 않고도 꽤 정확한 추정을 할 수 있게 도와주기도 한다.","tags":[{"name":"베이지안 통계","slug":"베이지안-통계","permalink":"https://the-masked-developer.github.io/tags/%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%ED%86%B5%EA%B3%84/"}],"categories":[{"name":"베이지안 통계","slug":"베이지안-통계","permalink":"https://the-masked-developer.github.io/categories/%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%ED%86%B5%EA%B3%84/"}]},{"title":"Nginx 간단 개념 (why 중심)","date":"2021-11-15T04:31:04.000Z","path":"wiki/nginx-why/","text":"Nginx 보다 먼저 만들어진 Apache server1995년에 만들어진 Apache 는 많은 클라이언트의 요청을 처리할 수 있게 해주어 많이 사용되었으며, 확장성 과 안정성 이 매우 뛰어남. client 의 요청이 들어올 경우, 미리 fork 해 둔 Process Pool 에서 해당 커넥션을 처리할 Process 를 할당하고 그 프로세스가 요청을 처리하게 됨. (초과 될 경우 fork) 이런 특징은 한 서버가 여러 요청을 처리할 수 있게 만들었으며 개발자들이 다양한 모듈을 작성할 수 있게 했다고 함. C10k 시대와 ApacheC10k (10,000 Connection problem): 스마트폰의 등장과 인터넷의 활성화로 점점 client 가 많아지면서, 다음과 같은 문제점들에 직면하게 됨. TCP Connection 생성의 비용은 비싸다. 그러므로 KeepAlive 로 client 들은 Connection 을 끊지 않기 시작했다. Process 생성에는 메모리가 필요하다. Connection 의 수 = 메모리 필요량이 비례하면서 부족지기 시작했다. Request 를 처리하기 위해 Process 를 switching 하는 context switching 비용도 비싸다. 그래서 위 문제를 해결하고자 Apache 진영도 많은 고민을 하고 개선해나가고 있으며, 새로운 구조로 만들어진 게 Nginx 이다. Nginx 의 특장점Event Driven 구조이다. Nginx 에서 Event 란, Connection 의 생성, 제거, 처리 등을 말한다. Nginx 에는 Master Process 와 그것이 생성하는 Worker Process 가 있는데, Worker 는 Event 를 기다렸다가 처리한다. 처리는 Worker Process 의 Thread Pool 의 Thread 에게 맡기므로, 혹시 Event 늖 Blocking 이므로 처리중 I/O 등으로 오래걸리는 Event 가 있을 경우도 동시에 처리가 가능하다. 개략적인 구조의 변경은 위와 같고, 정리해서/ 추가적으로 특장점은 다음과 같다. 하나의 Worker Process 가 많은 Connection Event 을 처리할 수 있게 되었다. 적은 Worker Process 의 수를 가지기 때문에, (보통 코어개수) 메모리 사용량이 적다. (요청이 늘어도 Thread stack 크기 정도의 메모리 추가사용) I/O 가 많이 필요한 Blocking 작업들 또한 효율적으로 처리하게 되었다. Worker Process 를 새로 띄우고 Event 를 맡기면 되므로, 동적으로 설정변경이 가능하게 되었다. (최 앞단에서 로드밸런싱등을 수행할 때 Single Point 잉므로 이 기능은 매우 핵심) Nginx 의 기능https://nginx.org/en/ 정적파일 서빙, 및 오토인덱싱, open fd 캐싱? 등 리버스 프록시서버로써 동작 및 캐싱. 로드밸런싱, fault tolerance filter 로써, 데이터 압축, 변환, … SSL Termination - SSL 처리 부담을 로직서버에 안가게끔 앞단에서 처리 CORS / HTTP/2 등 참고자료 Nginx official docs 우아한 형제들 10분 테코톡","tags":[],"categories":[{"name":"프로그래밍","slug":"프로그래밍","permalink":"https://the-masked-developer.github.io/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"}]},{"title":"자료구조와 객체 (1) - 추상화와 다형성","date":"2021-11-12T20:45:26.000Z","path":"wiki/struct-obj-1-poly/","text":"추상화의 수준의 차이자료구조는 데이터를 노출하고 객체는 추상화한다. 이 차이는 아래의 다형성의 차이를 만들어내게 된다. 다형성의 차이아래와 같은 두 클래스 Rectangle 와 Circle 이 있을 때, 123456789101112public class Rectangle &#123; public float x1; public float x2; public float y1; public float y2;&#125;public class Circle &#123; public float x; public float y; public float radius;&#125; 만약 두 클래스의 면적을 구하고 싶다면? 만약 두 클래스를 자료구조로 취급한다면 (struct?) 아래와 같은 helper class 를 만들게 된다. 1234567891011public class GeometryHelper &#123; public double calcArea(Object obj) &#123; if (obj instanceof Rectangle) &#123; // ... &#125; else if (obj instanceof Circle) &#123; // ... &#125; throw new Exception(&quot;No match&quot;); &#125;&#125; 코드는 객체지향이라기보다는 절차지향적이다. 만약 위 코드를 객체지향적으로 만든다면, Shape 와 같은 인터페이스를 정의하거나 상속받은 후 getArea() 와 같은 method 를 구현한다. 1234567891011121314151617181920public class Rectangle implements Shape &#123; public float x1; public float x2; public float y1; public float y2; public float getArea() &#123; // ... &#125;&#125;public class Circle implements Shape &#123; public float x; public float y; public float radius; public float getArea() &#123; // ... &#125;&#125; 위 코드는 객체지향적이 되었다. 이제 새로운 함수(둘레 또는 지름) 를 추가하거나 새로운 도형(타입)이 생길 수도 있다. 새로운 함수를 만들 때, 객체지향적인 코드는 모든 객체에 그 함수를 구현해야 한다. 즉 모든 객체에 대한 수정이 불가피하다. 새로운 도형을 만들 때는 기존 도형은 내버려 두어도 괜찮다. 절차지향적인 코드에서는 기존 체는 내버려둔채 새로운 함수를 구현하면 된다. 반면 새로운 도형을 만들 때는 모든 함수들에서 고려해야 한다. 객체지향적인 코드에서는 모든 도형에 새로운 함수를 추가하거나 새로운 도형을 만들면 된다. 즉, 기존 함수들은 수정할 일이 없다. 절자지향적인 코드에서는 GeometryHelper 에 함수만 수정하거나 추가하면 된다. 즉 기존 도형(자료구조) 를 변경하지 않는다. 결론 절차적인 코드는 새로운 자료구조를 추가하기 어렵다. (모든 함수를 고쳐야 한다.) 반면 객체지향적인 코드는 새로운 함수를 추가하기 어렵다. (모든 클래스를 고쳐야한다.) 위 두가지는 정확히 Trade off 관계로, 때로는 객체보다 단순한 자료구조와 절차지향적인 코드가 더 효과적일 수도 있다. 그러므로, 앞으로 새로운 타입이 필요해질 것 같은 지, 함수가 필요하게 될 지를 고려해서 절하게 사용하면 된다.","tags":[],"categories":[{"name":"프로그래밍","slug":"프로그래밍","permalink":"https://the-masked-developer.github.io/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"}]},{"title":"NestJS dynamic module 직접 만들어보기","date":"2021-11-12T10:00:22.000Z","path":"wiki/[nestjs]dynamic-module/","text":"소개 A module is a class annotated with a @Module() decorator. The @Module() decorator provides metadata that Nest makes use of to organize the application structure. Nest 에서는 @Module() 데코레이터를 이용해 모듈 클래스를 생성하고 등록할 수 있다. 특히 동적 모듈 (Dynamic Module) 기능을 이용해서 Provider 를 동적으로 등록하고 구성 할 수 있는 커스텀 가능한 모듈을 쉽게 만들 수 있다. Nestjs에서 공식적으로 지원하는 JwtModule 을 분석해보고, 직접 만들어보는 과정을 정리해보자. 분석 먼저 JwtModule 이 어떻게 사용되는지 살펴보겠다. 123456789101112131415161718// auth.module.ts// https://github.com/nestjs/nest/blob/master/sample/19-auth-jwt/src/auth/auth.module.tsimport &#123; JwtModule &#125; from &#x27;@nestjs/jwt&#x27;;@Module(&#123; imports: [ UsersModule, PassportModule, JwtModule.register(&#123; secret: jwtConstants.secret, signOptions: &#123; expiresIn: &#x27;60s&#x27; &#125;, &#125;), ], providers: [AuthService, LocalStrategy, JwtStrategy], exports: [AuthService],&#125;)export class AuthModule &#123;&#125; NestJS 공식 샘플 코드중 JwtModule 이 등록되는 부분이다. module.register(options) 와 같은 형태로 사용할 모듈의 static method 를 옵션과 함께 호출하여동적으로 프로바이더를 생성한다. 위와 같이 등록해주면 아래와 같이 jwtService 를 이용할 수 있게 된다 123456789101112131415161718192021222324252627282930// auth.service.ts// https://github.com/nestjs/nest/blob/master/sample/19-auth-jwt/src/auth/auth.service.tsimport &#123; Injectable &#125; from &#x27;@nestjs/common&#x27;;import &#123; UsersService &#125; from &#x27;../users/users.service&#x27;;import &#123; JwtService &#125; from &#x27;@nestjs/jwt&#x27;;@Injectable()export class AuthService &#123; constructor( private readonly usersService: UsersService, private readonly jwtService: JwtService, ) &#123;&#125; async validateUser(username: string, pass: string): Promise&lt;any&gt; &#123; const user = await this.usersService.findOne(username); if (user &amp;&amp; user.password === pass) &#123; const &#123; password, ...result &#125; = user; return result; &#125; return null; &#125; async login(user: any) &#123; const payload = &#123; username: user.username, sub: user.userId &#125;; return &#123; access_token: this.jwtService.sign(payload), &#125;; &#125;&#125; 그럼 이제 JwtModule 이 어떻게 정의되어있는지 살펴보자. https://github.com/nestjs/jwt/tree/master/lib 폴더구조는 아래와 같다. 1234567lib ├── interfaces ├── index.ts ├── jwt.constants.ts ├── jwt.module.ts ├── jwt.providers.ts └── jwt.service.ts 가장 먼저 등록되는 부분을 담당하는 jwt.module.ts 파일을 살펴보자. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import &#123; DynamicModule, Module, Provider &#125; from &#x27;@nestjs/common&#x27;;import &#123; JwtModuleAsyncOptions, JwtModuleOptions, JwtOptionsFactory&#125; from &#x27;./interfaces/jwt-module-options.interface&#x27;;import &#123; JWT_MODULE_OPTIONS &#125; from &#x27;./jwt.constants&#x27;;import &#123; createJwtProvider &#125; from &#x27;./jwt.providers&#x27;;import &#123; JwtService &#125; from &#x27;./jwt.service&#x27;;@Module(&#123; providers: [JwtService], exports: [JwtService]&#125;)export class JwtModule &#123; static register(options: JwtModuleOptions): DynamicModule &#123; return &#123; module: JwtModule, providers: createJwtProvider(options) &#125;; &#125; static registerAsync(options: JwtModuleAsyncOptions): DynamicModule &#123; return &#123; module: JwtModule, imports: options.imports || [], providers: this.createAsyncProviders(options) &#125;; &#125; private static createAsyncProviders( options: JwtModuleAsyncOptions ): Provider[] &#123; if (options.useExisting || options.useFactory) &#123; return [this.createAsyncOptionsProvider(options)]; &#125; return [ this.createAsyncOptionsProvider(options), &#123; provide: options.useClass, useClass: options.useClass &#125; ]; &#125; private static createAsyncOptionsProvider( options: JwtModuleAsyncOptions ): Provider &#123; if (options.useFactory) &#123; return &#123; provide: JWT_MODULE_OPTIONS, useFactory: options.useFactory, inject: options.inject || [] &#125;; &#125; return &#123; provide: JWT_MODULE_OPTIONS, useFactory: async (optionsFactory: JwtOptionsFactory) =&gt; await optionsFactory.createJwtOptions(), inject: [options.useExisting || options.useClass] &#125;; &#125;&#125; 이 중 NestJS 공식 샘플에서 사용된 register method 와 관련된 부분을 살펴보면 아래와 같다 123456789101112131415161718@Module(&#123; providers: [JwtService], exports: [JwtService]&#125;)export class JwtModule &#123; static register(options: JwtModuleOptions): DynamicModule &#123; return &#123; module: JwtModule, providers: createJwtProvider(options) &#125;; &#125;&#125; export function createJwtProvider(options: JwtModuleOptions): any[] &#123; return [&#123; provide: JWT_MODULE_OPTIONS, useValue: options || &#123;&#125; &#125;];&#125;export const JWT_MODULE_OPTIONS = &#x27;JWT_MODULE_OPTIONS&#x27;; register 함수는 JwtService 가 provide 되고, export 된 JwtModule 을 사용할 수 있도록NestJS 모듈 객체 형태로 리턴해준다. 이때 사용자가 입력한 옵션을 파라미터로 받아 동적 프로바이더를 생성해 주기 때문에 이 모듈을 사용하는 모든 곳에서 같은 암호화 옵션, 비밀키 등을 공유 할 수 있게 해준다. 또한, registerAsync 라는 다른 등록 구문을 활용해서 useClass, useExisting, useFactory 같은 구문을 이용해 다른 클래스를 추가로 이용하거나, 이미 존재하는 프로바이더를 alias 해서 사용하거나, 비동기적으로 생성 또한 가능하다. 12345678910111213141516171819202122232425262728293031323334353637383940static registerAsync(options: JwtModuleAsyncOptions): DynamicModule &#123; return &#123; module: JwtModule, imports: options.imports || [], providers: this.createAsyncProviders(options) &#125;; &#125; private static createAsyncProviders( options: JwtModuleAsyncOptions ): Provider[] &#123; if (options.useExisting || options.useFactory) &#123; return [this.createAsyncOptionsProvider(options)]; &#125; return [ this.createAsyncOptionsProvider(options), &#123; provide: options.useClass, useClass: options.useClass &#125; ]; &#125; private static createAsyncOptionsProvider( options: JwtModuleAsyncOptions ): Provider &#123; if (options.useFactory) &#123; return &#123; provide: JWT_MODULE_OPTIONS, useFactory: options.useFactory, inject: options.inject || [] &#125;; &#125; return &#123; provide: JWT_MODULE_OPTIONS, useFactory: async (optionsFactory: JwtOptionsFactory) =&gt; await optionsFactory.createJwtOptions(), inject: [options.useExisting || options.useClass] &#125;; &#125; Dynamic 모듈을 만들기 위한 필요 요건을 정리해보면 아래와 같다. 모듈을 등록할 수 있는 static register, static registerAsync 메소드가 포함된 모듈 클래스 모듈에서 제공할 서비스로직이 포함된 주입 가능한 서비스 클래스 동적으로 넘겨지는 객체를 주입받아 사용할 수 있도록 미리 선언된 상수와 동적 프로바이더 생성 로직 직접 만들어보기 NodeJS 에서 기본으로 제공하는 util 라이브러리중 inspect 라는 메소드가 있다. https://nodejs.org/api/util.html#utilinspectobject-options 이를 래핑하는 동적 모듈을 만들어보자 The util.inspect() method returns a string representation of object that is intended for debugging. The output of util.inspect may change at any time and should not be depended upon programmatically. Additional options may be passed that alter the result. util.inspect() will use the constructor’s name and/or @@toStringTag to make an identifiable tag for an inspected value. 이 메소드는 javscript object 디버깅하거나, 읽기 쉽게 변형해서 string 으로 리턴해주는 유틸 메소드다. 아래 예시처럼 동작한다. 1234567891011121314151617181920const util = require(&#x27;util&#x27;);const o = &#123; a: [1, 2, [[ &#x27;Lorem ipsum dolor sit amet,\\nconsectetur adipiscing elit, sed do &#x27; + &#x27;eiusmod \\ntempor incididunt ut labore et dolore magna aliqua.&#x27;, &#x27;test&#x27;, &#x27;foo&#x27;]], 4], b: new Map([[&#x27;za&#x27;, 1], [&#x27;zb&#x27;, &#x27;test&#x27;]])&#125;;console.log(util.inspect(o, &#123; compact: true, depth: 5, breakLength: 80 &#125;));// &#123; a:// [ 1,// 2,// [ [ &#x27;Lorem ipsum dolor sit amet,\\nconsectetur [...]&#x27;, // A long line// &#x27;test&#x27;,// &#x27;foo&#x27; ] ],// 4 ],// b: Map(2) &#123; &#x27;za&#x27; =&gt; 1, &#x27;zb&#x27; =&gt; &#x27;test&#x27; &#125; &#125; 먼저, 필요 요건들을 만들기 전에 어떻게 등록할지 정의해보자. 1234567891011121314151617import &#123; Module &#125; from &#x27;@nestjs/common&#x27;;import &#123; InspectModule &#125; from &#x27;./inspect/inspect.module&#x27;;@Module(&#123; imports: [ UsersModule, PassportModule, JwtModule.register(&#123; secret: jwtConstants.secret, signOptions: &#123; expiresIn: &#x27;60s&#x27; &#125;, &#125;), InspectModule.register(&#123;showHidden: false, depth: null, colors: true&#125;), ], providers: [AuthService, LocalStrategy, JwtStrategy], exports: [AuthService],&#125;)export class AuthModule &#123;&#125; 맨 처음 JwtModule 을 어떻게 등록하는지 살펴 봤던 그 파일이다.JwtModule 아래에 InspectModule을 등록하는 부분을 추가해주었다. 이제 필요요건들을 하나씩 만들어보자. 모듈을 등록할 수 있는 static register, static registerAsync 메소드가 포함된 모듈 클래스 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import &#123; DynamicModule, Module, Provider &#125; from &#x27;@nestjs/common&#x27;;import &#123; INSPECT_MODULE_OPTION &#125; from &#x27;./inspect.constants&#x27;;import &#123; InspectAsyncOptions, InspectOptions, InspectOptionsFactory &#125; from &#x27;./inspect.interfaces&#x27;;import &#123; createInspectProviders &#125; from &#x27;./inspect.providers&#x27;;import &#123; InspectService &#125; from &#x27;./inspect.service&#x27;;@Module(&#123; providers: [InspectService], exports: [InspectService]&#125;)export class InspectModule &#123; public static register(options: InspectOptions): DynamicModule &#123; return &#123; module: InspectModule, providers: createInspectProviders(options), &#125;; &#125; static registerAsync(options: InspectAsyncOptions): DynamicModule &#123; return &#123; module: InspectModule, imports: options.imports || [], providers: this.createAsyncProviders(options) &#125;; &#125; private static createAsyncProviders( options: InspectAsyncOptions ): Provider[] &#123; if (options.useExisting || options.useFactory) &#123; return [this.createAsyncOptionsProvider(options)]; &#125; return [ this.createAsyncOptionsProvider(options), &#123; provide: options.useClass, useClass: options.useClass &#125; ]; &#125; private static createAsyncOptionsProvider( options: InspectAsyncOptions ): Provider &#123; if (options.useFactory) &#123; return &#123; provide: INSPECT_MODULE_OPTION, useFactory: options.useFactory, inject: options.inject || [] &#125;; &#125; return &#123; provide: INSPECT_MODULE_OPTION, useFactory: async (optionsFactory: InspectOptionsFactory) =&gt; await optionsFactory.createInspectOptions(), inject: [options.useExisting || options.useClass] &#125;; &#125;&#125; 모듈에서 제공할 서비스로직이 포함된 주입 가능한 서비스 클래스 123456789101112import &#123; Inject, Injectable &#125; from &#x27;@nestjs/common&#x27;;import util, &#123; InspectOptions &#125; from &#x27;util&#x27;;import &#123; INSPECT_MODULE_OPTION &#125; from &#x27;./inspect.constants&#x27;;@Injectable()export class InspectService &#123; constructor(@Inject(INSPECT_MODULE_OPTION) private readonly options: InspectOptions) &#123;&#125; inspect(object: any): string &#123; return util.inspect(object, this.options) &#125;&#125; 동적으로 넘겨지는 객체를 주입받아 사용할 수 있도록 미리 선언된 상수와 동적 프로바이더 생성 로직 123456789101112import &#123; Provider &#125; from &quot;@nestjs/common&quot;;import &#123; INSPECT_MODULE_OPTION &#125; from &quot;./inspect.constants&quot;;import &#123; InspectOptions &#125; from &quot;./inspect.interfaces&quot;;export function createInspectProviders(options: InspectOptions): Provider[] &#123; return [ &#123; provide: INSPECT_MODULE_OPTION, useValue: options || &#123;&#125;, &#125;, ]; &#125; 123456import &#123; ModuleMetadata, Type &#125; from &#x27;@nestjs/common&#x27;;import &#123; InspectOptions &#125; from &#x27;util&#x27;;const INSPECT_MODULE_OPTION = &quot;INSPECT_MODULE_OPTION&quot;export &#123; INSPECT_MODULE_OPTION &#125; sample src : https://github.com/seonjl/nestjs-inspect/tree/main/sample 참고한 레퍼런스 https://dev.to/nestjs/advanced-nestjs-how-to-build-completely-dynamic-nestjs-modules-1370","tags":[{"name":"nestjs","slug":"nestjs","permalink":"https://the-masked-developer.github.io/tags/nestjs/"}],"categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://the-masked-developer.github.io/categories/nodejs/"}]},{"title":"타입스크립트로 좋은 테스트 작성하기","date":"2021-11-10T13:22:22.000Z","path":"wiki/wellmade-test-writing/","text":"좋은 테스트 코드를 작성하는 원칙깨끗한 테스트 코드는 신뢰할 수 있는 개발과 배포를 가능하게 합니다! 직관적이고 이해가 되는 테스트 이름으로 구성하기 테스트 이름을 추상적으로 쓰지마세요. 정확히 무엇을 테스트 하는지 명시적으로 작성하세요. 🟥 안좋은 예 12345describe(&quot;주문 검증&quot;, () =&gt; &#123; const postOrder = &#123; ... &#125;; const order = await service.addOrder(postOrder); expect(order).toEqual()&#125;); 🟦 좋은 예 12345describe(&quot;정상적인 주문이 추가되는지 검증합니다.&quot;, () =&gt; &#123; const postOrder = &#123; ... &#125;; const order = await service.addOrder(postOrder); expect(order).toEqual()&#125;); 글로벌 테스트 픽스처를 작성하지 마세요 복잡하고 이해하기 힘든 테스트 픽스처는 쉽게 추론하기가 힘듭니다. 테스트의 결합도가 증가하는 건 좋지 않습니다. 반복 사용되는 픽스처가 있다면, 외부에 static 팩토리 메소드를 만드세요. 🟥 안좋은 예 123456789beforeAll(async () =&gt; &#123; const user = await service.addUser(); const money = await service.addMoney(user); await service.addOrder(user, money);&#125;);describe(&quot;주문을 취소합니다&quot;, () =&gt; &#123; const order = await service.deleteOrder(); expect(order.status).toBe(&quot;canceled&quot;)&#125;); 🟦 좋은 예 123456789101112class TestFactory &#123; static create() &#123; return new ~~() &#125;&#125;describe(&quot;정상적인 주문이 추가되는지 검증합니다.&quot;, () =&gt; &#123; const user = TestFactory.createUser() const money = TestFactory.createMoney() const order = await service.addOrder(user, money); expect(order.status).toBe(&quot;canceled&quot;)&#125;); 테스트를 반드시 격리시키세요 하나의 테스트는 반드시 격리되어 실행되어 외부에 영향을 받지 않아야 합니다. 🟥 안좋은 예 1234567891011let addOrderdescribe(&quot;주문을 추가합니다&quot;, () =&gt; &#123; const order = await service.addOrder(); addOrder = order; expect(order.status).toBe(&quot;added&quot;)&#125;);describe(&quot;주문을 취소합니다&quot;, () =&gt; &#123; const order = await service.deleteOrder(addOrder); expect(order.status).toBe(&quot;canceled&quot;)&#125;); 🟦 좋은 예 12345678910describe(&quot;주문을 추가합니다&quot;, () =&gt; &#123; const order = await service.addOrder(); expect(order.status).toBe(&quot;added&quot;)&#125;);describe(&quot;주문을 취소합니다&quot;, () =&gt; &#123; const addOrder = await service.addOrder(); const order = await service.deleteOrder(addOrder); expect(order.status).toBe(&quot;canceled&quot;)&#125;);","tags":[{"name":"글쓰기","slug":"글쓰기","permalink":"https://the-masked-developer.github.io/tags/%EA%B8%80%EC%93%B0%EA%B8%B0/"}],"categories":[{"name":"글쓰기","slug":"글쓰기","permalink":"https://the-masked-developer.github.io/categories/%EA%B8%80%EC%93%B0%EA%B8%B0/"}]},{"title":"Supervisord 찍어먹기","date":"2021-11-09T14:05:19.000Z","path":"wiki/Supervisord/","text":"Supervisorsupervisord 또는 supervisordctl command 는 기본적으로 다음과 같은 경로에서 순서대로 supervisord.conf 을 찾는다. ../etc/supervisord.conf (Relative to the executable) ../supervisord.conf (Relative to the executable) $CWD/supervisord.conf $CWD/etc/supervisord.conf /etc/supervisord.conf /etc/supervisor/supervisord.conf (since Supervisor 3.3.0) 우선 다음과 같이 예제 어플리케이션을 생성했다. 123456789#!/usr/bin/env python3from time import sleepprint(&quot;Starting application...&quot;)for i in range(5): sleep(1) print(f&quot;[&#123;i&#125;] working...&quot;) 123456789# docker-compose.ymlversion: &quot;3.8&quot;services: app: image: python:latest volumes: - &quot;./app.py:/app.py&quot; command: /app.py 실행 할 경우. 12345678910~/Dropbox/box/demos/supervisord master* ⇣⇡ ❯ dc upStarting supervisord_app_1 ... doneAttaching to supervisord_app_1app_1 | Starting application...app_1 | [0] working...app_1 | [1] working...app_1 | [2] working...app_1 | [3] working...app_1 | [4] working...supervisord_app_1 exited with code 0 작업을 마친 후 자동으로 종료된다. 이제 supervisord.conf 를 생성한다. 파일 형식은 윈도우즈의 ini 로써 각 섹션은 [HEADER] 로 나타내며섹션안에 Key=Value 페어가 들어간다. 공식 홈페이지를 보고 처음부터 한땀한땀 해도 되겠지만 공식 레포의 샘플 로 시작하는게 좋을듯 하다.(set filetype=dosini 로 문법 하이라이팅을 켤 수 있다.) HTTP 서버를 돌릴게 아니므로, unix_http_server 및 inet_http_server 는 지운다. supervisord 는 supervisord 프로세스에 대한 설정이므로 냅둔다. rpc 또한 해당사항이 없으므로 rpcinterface 도 지운다. supervisorctl 의 경우 쉘 커맨드 설정을 위함인데, 유저별로 권한을 주는 등 설정할 필요는 없으므로 지운다. include 는 다른 supervisord.conf 파일을 포함시킬 수 있게 해준다. 지운다. group 은 program 들을 묶을 수 있게 해준다. 지운다. eventlistener 는 supervisor’s event system. 로부터 이벤트를 받을 수 있는 프로그램이다. 지운다. supervisor 는 python 으로 구현되어 있으므로 pip 로 설치한다. 그러므로 Dockerfile 을 생성하고설치하도록 했다. (버퍼를 꺼줘야 컨테이너 로그가 바로 보인다.) 12345FROM python:latestENV PYTHONUNBUFFERED 1RUN pip install supervisor supervisor.conf 파일을 docker-compose.yml 의 volume 에 추가한다. 12345678services: app: build: ./ volumes: - &quot;./app.py:/app.py&quot; - &quot;./supervisord.conf:/etc/supervisord.conf&quot; command: supervisord 이렇게 실행 할 경우 nodaemon 값을 true 로 해줘야한다. 12[program:app] command=/app.py ; the program (relative uses PATH, can take args) 이제 다시 실행할 경우 app 의 로그는 보이지 않지만 정상적으로 종료되었음을 알 수 있다. 1234567app_1 | /usr/lib/python3/dist-packages/supervisor/options.py:474: UserWarning: Supervisord is running as root and it is searching for its configuration file in default locations (including its current working directory); you probably want to specify a &quot;-c&quot; argument specifying an absolute path to a configuration file for improved security.app_1 | self.warnings.warn(app_1 | 2021-09-01 10:01:47,610 CRIT Supervisor is running as root. Privileges were not dropped because no user is specified in the config file. If you intend to run as root, you can set user=root in the config file to avoid this message.app_1 | 2021-09-01 10:01:47,613 INFO supervisord started with pid 1app_1 | 2021-09-01 10:01:48,616 INFO spawned: &#x27;app&#x27; with pid 8app_1 | 2021-09-01 10:01:49,707 INFO success: app entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)app_1 | 2021-09-01 10:01:51,714 INFO exited: app (exit status 0; expected) 로그는 stdout_logfile 을 통해서 std 또는 file 에 쓰면 된다. 123[supervisord] stdout_logfile=/dev/stdout ; stdout log path, NONE for none; default AUTO stdout_logfile_maxbytes=0 ; max # logfile bytes b4 rotation (default 50MB) program 의 auturestart 의 기본값은 unexpected 로써 exit code 0 이 아닌 경우 재시작한다. 그러므로app.py 에서 sys.exit(1) 로 비정상 종료 코드를 내보낸다면 supervisord 가 프로세스를 재시작해준다. 123456789app_1 | 2021-09-01 12:09:11,750 INFO spawned: &#x27;app&#x27; with pid 1819app_1 | Starting application...app_1 | [0] working...^[app_1 | 2021-09-01 12:09:12,783 INFO success: app entered RUNNING state, process has stayed up for &gt; than 1 seconds (startsecs)app_1 | [1] working...app_1 | [2] working...app_1 | 2021-09-01 12:09:14,862 INFO exited: app (exit status 1; not expected)app_1 | 2021-09-01 12:09:15,869 INFO spawned: &#x27;app&#x27; with pid 1820","tags":[],"categories":[{"name":"프로그래밍","slug":"프로그래밍","permalink":"https://the-masked-developer.github.io/categories/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/"}]},{"title":"글 작성 가이드","date":"2021-11-09T13:22:22.000Z","path":"wiki/guide/","text":"글 작성 가이드사용할 수 있는 마크다운 문법 및 추가적인 플러그인 사용법에 관해서 기술 emphasis: keyword highlight.js 1234567def fib(n): a, b = 0, 1 while a &lt; n: print(a, end=&#x27; &#x27;) a, b = b, a+b print()fib(1000) 123456type Map struct &#123; mu Mutex read atomic.Value dirty map[interface&#123;&#125;]*entry misses int&#125; Table Header 1 Header 2 Header 3 Key 1 Value 1 Comment 1 Key 2 Value 2 Comment 2 Key 3 Value 3 Comment 3 H2H3번호가 있는 목록 A B C H3번호가 없는 목록 A B C 이미지 LaTex$$\\Gamma _ { \\epsilon } ( x ) = [ 1- e ^ { - 2\\pi \\epsilon } ] ^ { 1- x } \\prod _ { n = 0} ^ { \\infty } \\frac { 1- \\operatorname{exp} ( - 2\\pi \\epsilon ( n + 1) ) } { 1- \\operatorname{exp} ( - 2\\pi \\epsilon ( x + n ) ) }$$ $$\\left( \\begin{array} c t ^ { \\prime } \\ x ^ { \\prime } \\ y ^ { \\prime } \\ z ^ { \\prime } \\end{array} \\right) = \\left( \\begin{array} { c c c c } { \\gamma } &amp; { - \\gamma \\beta } &amp; { 0 } &amp; { 0 } \\ { - \\gamma \\beta } &amp; { \\gamma } &amp; { 0 } &amp; { 0 } \\ { 0 } &amp; { 0 } &amp; { 1 } &amp; { 0 } \\ { 0 } &amp; { 0 } &amp; { 0 } &amp; { 1 } \\end{array} \\right) \\left( \\begin{array} c t \\ x \\ y \\ z \\end{array} \\right)$$ $$6 \\mathrm { CO } _ { 2 } + 6 \\mathrm { H } _ { 2 } \\mathrm { O } \\rightarrow \\mathrm { C } _ { 6 } \\mathrm { H } _ { 12 } \\mathrm { O } _ { 6 } + 6 \\mathrm { O } _ { 2 }$$ mermaid flowchartsequenceDiagram participant Alice participant Bob Alice-&gt;&gt;John: Hello John, how are you? loop Healthcheck John-&gt;&gt;John: Fight against hypochondria end Note right of John: Rational thoughts &lt;br/&gt;prevail... John--&gt;&gt;Alice: Great! John-&gt;&gt;Bob: How about you? Bob--&gt;&gt;John: Jolly good! gantt dateFormat YYYY-MM-DD title Adding GANTT diagram to mermaid section A section Completed task :done, des1, 2014-01-06,2014-01-08 Active task :active, des2, 2014-01-09, 3d Future task : des3, after des2, 5d Future task2 : des4, after des3, 5d classDiagram Class01 &lt;|-- AveryLongClass : Cool Class03 *-- Class04 Class05 o-- Class06 Class07 .. Class08 Class09 --&gt; C2 : Where am i? Class09 --* C3 Class09 --|&gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla Class08 &lt;--&gt; C2: Cool label note Info markdown Warning markdown etc markdown labelinfo warning primary checkbox checkbox checkbox jsbutton 이미지 분할 참조나무위키 태그[1]： 나무위키 태그[2]。 수장형이 좋아할듯 ↩맞지? ↩","tags":[{"name":"글쓰기","slug":"글쓰기","permalink":"https://the-masked-developer.github.io/tags/%EA%B8%80%EC%93%B0%EA%B8%B0/"}],"categories":[{"name":"글쓰기","slug":"글쓰기","permalink":"https://the-masked-developer.github.io/categories/%EA%B8%80%EC%93%B0%EA%B8%B0/"}]},{"title":"CORS 이해하기","date":"2021-03-09T13:22:22.000Z","path":"wiki/cors/","text":"CORS 란교차 출처 리소스 공유 ( Cross-Origin Resource Sharing) 을 의미한다. 여기서 출처 ( Origin ) 이란 https://api.server.com/blah/blah 와 같은 URL 에서 프로토콜 + 호스트 + 포트 를 전부 합친 서버의 위치, 리소스의 출처를 의미한다 포트는 기본 프로토콜 포트번호가 정해져 있기 때문에 생략할 수 있으며, 만약 명시적으로 포트가 포함되어 있다면 포트번호까지 모두 일치해야 한다. SOP ( Same-Origin Policy )다른 출처로의 리소스 요청을 제한하는 보안 정책 다른 출처의 리소스를 마음대로 가져와서 사용할 수 있는 웹 환경은 매우 보안에 취약하다.그래서, 이러한 SOP 정책을 도입하여 안전한 환경을 구축 할 수 있지만, 다른 출처에 리소스를 가져와서 사용하는 일은 분명히 필요한 일이기에, 예외적으로 이를 허용해 줄 필요가 있다. 그 중 하나가 바로 CORS 정책을 지킨 리소스 요청이다. 이러한 보안 정책은 브라우저에 내장되어 있기 때문에 브라우저를 통하지 않고 서버 간 통신을 할 떄는 적용되지 않는다. CORS 의 기본적인 동작 방식클라이언트는 HTTP 프로토콜을 사용하여 요청을 보내게 되고,이때 브라우저는 요청 헤더에 Origin 필드에 출처를 함께 담아 보낸다. 이 요청을 받은 서버는 응답 헤더의 Access-Control-Allow-Origin 이라는 키에 허용된 출처 값을 내려주고, 응답을 받은 브라우저는 자신이 보낸 요청의 Origin 과 서버가 보내준 응답의 Access-Control-Allow-Origin 을 비교해 본 후 유효성을 결정한다. 세가지 시나리오Preflight Request클라이언트가 요청을 보내기 전 브라우저가 보내게 되는 예비 요청이다 OPTIONS 메소드를 사용하여 서버가 어떤 것들을 허용하고, 어떤 것들을 금지하고 있는지에 대한 정보를 담아오게 된다. 브라우저는 이 요청을 검사한 후 본 요청의 송신 여부를 결정하게 된다. Simple Request브라우저가 예비요청을 보내지 않고 바로 본 요청을 보내는 경우도 있다. 아래에 특정한 조건을 만족하는 경우에 보내지게 된다. 요청의 메소드는 GET, HEAD, POST 중 하나여야 한다. Accept, Accept-Language, Content-Language, Content-Type, DPR, Downlink, Save-Data, Viewport-Width, Width를 제외한 헤더를 사용하면 안된다. 만약 Content-Type를 사용하는 경우에는 application/x-www-form-urlencoded, multipart/form-data, text/plain만 허용된다. Credentialed Request클라이언트에 요청에 자격증명모드가 include 일경우 발생한다. 이러한 경우 브라우저에 보안 정책이 한가지 더 추가 되게 된다. 브라우저는 쿠키정보나 인증과 관련된 헤더를 함부로 요청에 담지 않는다. 이러한 인증정보를 요청에 담을 수 있게 해주는 옵션이 바로 Request.credentails 옵션이다. same-origin (default) : 같은 출처 간 요청에만 인증 정보를 담을 수 있다. include : 모든 요청에 인증 정보를 담을 수 있다. omit : 모든 요청에 인증 정보를 담지 않는다 만약 다른 출처로 인증관련 헤더를 포함하여 요청을 보내고 싶다면credentials 의 include 모드를 사용하여 요청을 작성하여 보내야 한다. 이때 발생할 수 있는 두가지 정책 위반이 있다. 🚨 Access to fetch at ’https://api.server.com’ from origin ’http://localhost:8000’ has been blocked by CORS policy: The value of the ‘Access-Control-Allow-Origin’ header in the response must not be the wildcard ’*’ when the request’s credentials mode is ‘include’. 🚨 Access to XMLhttpRequest at ’https://api.server.com’ from origin ’http://localhost:8000’ has been blocked by CORS policy: Response to preflight request doesn’t pass access control check: The value of the ‘Access-Control-Allow-Credentials’ header in the response is ‘ ‘ which must be ‘true’ when the request’s credentials mode is ‘include’. The credentials mode of requests initiated by the XMLhttpRequest is controlled by the withCredentials attribute. * 를 Access-Control-Allow-Origin 헤더에 사용하면 안된다.응답 헤더에 반드시 Access-Control-Allow-Credentials: true 가 존재해야한다. 해결방법Access-Control-Allow-Origin 세팅하기nginx 설정에서 헤더값을 세팅해주거나 application 에서 세팅해주거나 하면 된다. * 로 전체를 허용해 주기 보다는 특정한 출처를 명시해주는 편이 좋다. Webpack Dev Server 로 리버스 프록싱하기devServer 에 proxy 옵션을 사용해 특정 주소로 프록싱을 해주면 브라우저는 내가 세팅한 특정주소를 Origin 으로 알고 요청을 보내기 때문에 CORS 정책을 지킨 것 처럼 브라우저를 속이고 통신할 수 있다. Hosts 파일 이용하기hosts 파일 위치 : C:\\Windows\\System32\\drivers\\etc 1127.0.0.1:8080 api.front.com 호스트파일을 위와 같이 변경하여 마치 같은 Origin 인 것처럼 속일 수 도 있다. 참고자료 CORS는 왜 이렇게 우리를 힘들게 하는걸까?","tags":[{"name":"web","slug":"web","permalink":"https://the-masked-developer.github.io/tags/web/"}],"categories":[{"name":"web","slug":"web","permalink":"https://the-masked-developer.github.io/categories/web/"}]},{"title":"리눅스에서 무슨무슨 so 파일이 없다고 할 때","date":"2021-03-09T13:22:22.000Z","path":"wiki/no-so-file/","text":"발단 nodejs 에서 oracledb 클라이언트를 다운받아 사용하려고 하면 항상 만나는 에러메시지가 있다. 123문제가 있으니이걸 보고 해결하시오https://oracle.github.io/node-oracledb/INSTALL.html#overview 나는 이러한 에러를 만났다. 12libclntsh.so: cannot open shared object file: No such file or directorylibaio.so.1: cannot open shared object file: No such file or directory so 파일은 shared object 파일이란 뜻으로 특정한 기능을 구현해 놓은 라이브러리 파일이다. 즉 내 컴퓨터에 저 파일이 없어서 오라클 클라이언트를 사용할 수 없다고 하는 문제인 것이다. 파일을 다운로드해서, 리눅스에서 라이브러리를 로딩하려면 ldconfig 라는 명령어를 사용해서, /etc/ld.so.conf 파일에 설정된 라이브러리 정보를/etc/ld.so.cache 파일로 만들어 준후 리눅스에 로더 가 라이브러리를 찾아 사용할 수 있도록 해주면 된다. 해결 💡 리눅스는 so 파일을 아래와 같은 순서로 찾는다system default 경로LD_LIBRARY_PATHbinary code 에 hard-coding 된 경로 시스템 디폴트 경로에 so 파일을 등록하는 방법을 알아보자. 1. system default 경로 이 값은 /etc/ld.so.conf 파일에 설정이 된 값이다. (일반적으로 /usr/local/bin 과 /usr/bin 이다) 12$ more /etc/ld.so.confinclude /etc/ld.so.conf.d/*conf /etc/ld.so.conf.d/*conf 에 해당하는 모든 파일의 내용을 포함하고 있다. 새로운 라이브러리를 리눅스에 등록하고 싶다면,추가할 라이브러리 파일의 경로를 저장한 파일을위와 같은 파일 이름 형식으로 만들어 위 경로에 저장해 주면 된다. 12345$ ls -al /usr/lib/oracle/19.6/client64/liblibclntsh.solib~~~lib~~~lib~~~ 12# /etc/ld.so.conf.d/oracle.conf/usr/lib/oracle/19.6/client64/lib 원하는 버전의 오라클 클라이언트를 위 경로에 다운 받은 후, 위와 같은 파일을 만들어 경로를 등록 해 준후 ldconfig 명령어로 등록하면 사용할 수 있다 +) libaio1 은 아래와같이 패키지매니저로 설치해 줄 수 있다. 1sudo apt-get install libaio1 ++) 라이브러리 파일에 실행권한이 있냐 없냐 여부는 상관이 없는것 같다. +++) 라이브러리 파일이 심볼릭 링크인지 여부는 당연히 상관이 없다. 참고한 자료들 리눅스에서 무슨무슨 so 파일이 없다고 할 때C++ - [펌] Linux에서 라이브러리 로딩","tags":[{"name":"linux","slug":"linux","permalink":"https://the-masked-developer.github.io/tags/linux/"}],"categories":[{"name":"linux","slug":"linux","permalink":"https://the-masked-developer.github.io/categories/linux/"}]}]}